[
  {
    "objectID": "02_finding-report.html",
    "href": "02_finding-report.html",
    "title": "NOTE: the result is based on the code base abb9a21, which is similar to the commit 69d61a9 in the main branch",
    "section": "",
    "text": "import scipy\nimport pickle\nimport yaml\nimport pandas as pd\nimport altair as alt\nfrom collections import Counter\n\n\ndef get_report(response):\n    report = []\n    for result in response.call_results:\n        if result.parsed_response:\n            resp = result.parsed_response['results']\n            for item in resp:\n                item['file'] = result.files_evaluated[0] \n                item['success'] = result.success\n                report.append(item)\n        else:\n            report.append({\n                'ID': '2.1', # FIXME\n                'Title': '',\n                'Requirement': '',\n                'Observation': '',\n                'Functions': [],\n                'Evaluation': '',\n                'Score': 0,\n                'file': result.files_evaluated[0],\n                'success': result.success\n            })\n    return pd.DataFrame(report)\n\ndef extract_file_and_scores(resp_path, verbose=False):\n    if verbose:\n        print(resp_path)\n    with open(resp_path, 'rb') as file:\n        response = pickle.load(file)\n    report = get_report(response)\n    df = (\n        report\n        .pivot(index='file', columns='ID', values='Score')\n        .rename_axis(None, axis=1)\n    )\n    df['success'] = report.groupby(['file'])['success'].all()\n    df['response_path'] = resp_path\n    return df.reset_index()\n\ndef generate_stat_plot(df_repo__stat, ground_truth=None, facet_col='repo', repo=None, id=None):\n    \"\"\"\n    Generate Stat plot across all repo and all checklist item\n    Optional to incorporate ground truth and select specific repo/checklist item\n    \"\"\"\n    if facet_col == 'repo':\n        x_col = 'id'\n        x_title = 'Checklist ID'\n    elif facet_col == 'id':\n        x_col = 'repo'\n        x_title = 'Repository'\n    \n    # the base chart\n    if repo:\n        df_repo__stat = df_repo__stat.query(f'repo == \"{repo}\"')\n    if id:\n        df_repo__stat = df_repo__stat.query(f'id == \"{id}\"')\n    \n    base = alt.Chart().transform_calculate(\n        min=\"max(0, datum.mean-datum.std)\",\n        max=\"min(1, datum.mean+datum.std)\"\n    )\n    \n    # generate the points\n    points = base.mark_point(\n        filled=True,\n        size=50,\n        color='black'\n    ).encode(\n        x=alt.X(f'{x_col}:O').axis(labelAngle=0).title(x_title),\n        y=alt.Y('mean:Q').scale(domainMin=0, domainMax=1).title('Score'),\n    )\n    \n    # generate the error bars\n    errorbars = base.mark_errorbar().encode(\n        x=f\"{x_col}:O\",\n        y=alt.Y(\"min:Q\").title('1 SD'),\n        y2=\"max:Q\"\n    )\n\n    plot = points + errorbars\n    \n    if ground_truth is not None:\n        # generate points of ground truth\n        if repo:\n            ground_truth = ground_truth.query(f'repo == \"{repo}\"')\n        if id:\n            ground_truth = ground_truth.query(f'id == \"{id}\"')\n        \n        df_repo__stat = pd.merge(df_repo__stat, ground_truth, how='left', on=['repo', 'id'])\n        \n        gt_points = alt.Chart().mark_point(\n            filled=True,\n            size=100,\n            color='green',\n            shape=\"diamond\"\n        ).encode(\n            x=alt.X(f'{x_col}:O'),\n            y=alt.Y('score:Q')\n        )\n\n        plot += gt_points\n\n    plot = alt.layer(\n                plot,\n                data=df_repo__stat\n            ).properties(\n                width=400,\n            ).facet(\n                column=f'{facet_col}',\n                columns=2\n            )\n\n    return plot\n\n\npreprocess data\n\nchecklist_ids = ['2.1', '3.2', '3.5', '4.2', '5.3', '6.1', '6.2']\n\n#result_path = '../draft/batch_run_results/record_combine.yml'\nresult_path = '../data/processed/batch_run/record_combine.yml'\nwith open(result_path, 'r') as file:\n    config = pd.DataFrame(yaml.safe_load(file))\n\n# prepare score data by repo, run, file\ntmp = [\n    extract_file_and_scores(path) for path in config['response_path'] # FIXME: excluded deepchem\n]\ntmp = pd.concat(tmp, axis=0).reset_index(drop=True)\n\nraw_df_repo_run_file = config.merge(tmp, on='response_path', how='left')\n\n\n# filter non-test files in qlib\ndf_repo_run_file = raw_df_repo_run_file.query('(repo != \"qlib\") | (file.str.contains(\"../data/raw/openja/qlib/tests/\"))')\n\n# prepare score data by repo, run\ndf_repo_run = df_repo_run_file.groupby(['repo', 'run']).agg({\n    id: ['max'] for id in checklist_ids\n})\ndf_repo_run.columns = [col[0] for col in df_repo_run.columns]\ndf_repo_run = df_repo_run.reset_index()\n\n# prepare statistics of scores by repo\ndf_repo__stat = df_repo_run.groupby(['repo']).agg({\n    id: ['mean', 'std', 'count'] for id in checklist_ids\n})\ndf_repo__stat = pd.melt(df_repo__stat.reset_index(), id_vars=[('repo', '')])\ndf_repo__stat.columns = ['repo', 'id', 'stat', 'value']\ndf_repo__stat = (\n    df_repo__stat.pivot(index=['repo', 'id'], columns='stat', values='value')\n    .reset_index()\n    .rename_axis(None, axis=1)\n)\n\n# prepare counting of scores by repo\ndf_repo__count = df_repo_run.groupby(['repo'])['2.1'].apply(Counter).reset_index()\nfor id in checklist_ids[1:]:\n    df_repo__count = df_repo__count.merge(\n        df_repo_run.groupby(['repo'])[id].apply(Counter).reset_index(),\n        on=['repo', 'level_1'],\n        how='outer'\n    )\n\ndf_repo__count = df_repo__count.fillna(0)\n\n\n\nRuns Quality\n\n1. Some non-test files are included in the evaluation\nFor example, the ./nanodet/nanodet/trainer/task.py\n\nraw_df_repo_run_file.query('repo == \"nanodet\"')['file'].unique()[:3]\n\narray(['../data/raw/openja/nanodet/nanodet/trainer/task.py',\n       '../data/raw/openja/nanodet/tests/test_configs/test_config.py',\n       '../data/raw/openja/nanodet/tests/test_data/test_batch_process.py'],\n      dtype=object)\n\n\n\n\n2. Evaluation on the file magenta/magenta/models/music_vae/data_test.py is always failed\n\ndf_repo_run_file[~df_repo_run_file.success]['file'].unique()\n\narray(['../data/raw/openja/magenta/magenta/models/music_vae/data_test.py',\n       '../data/raw/openja/paperless-ng/src/documents/tests/test_api.py'],\n      dtype=object)\n\n\n\n\n3. DeepSpeech, lightfm and magenta have the least (Python) test files\n\ndf_repo_run_file.query('run == 1').groupby(['repo'])['file'].count().reset_index()\n\n\n\n\n\n\n\n\nrepo\nfile\n\n\n\n\n0\nDeepSpeech\n3\n\n\n1\napollo\n14\n\n\n2\nlightfm\n7\n\n\n3\nmagenta\n8\n\n\n4\nmmf\n70\n\n\n5\nmycroft-core\n64\n\n\n6\nnanodet\n42\n\n\n7\npaperless-ng\n35\n\n\n8\nqlib\n31\n\n\n\n\n\n\n\n\n\n4. The test files are not always in a tests/ folder. Is it be good practice to always do that? Should it be one of the checklist item to ensure all tests placed under tests/ folder?\nFor example, magenta\n\ndf_repo_run_file.query('repo == \"magenta\"')['file'].unique()\n\narray(['../data/raw/openja/magenta/conftest.py',\n       '../data/raw/openja/magenta/magenta/common/state_util_test.py',\n       '../data/raw/openja/magenta/magenta/models/coconet/export_saved_model_test.py',\n       '../data/raw/openja/magenta/magenta/models/coconet/lib_data.py',\n       '../data/raw/openja/magenta/magenta/models/music_vae/data_test.py',\n       '../data/raw/openja/magenta/magenta/models/onsets_frames_transcription/create_dataset_lib_test.py',\n       '../data/raw/openja/magenta/magenta/models/score2perf/datagen_beam_test.py',\n       '../data/raw/openja/magenta/magenta/pipelines/pipeline_test.py'],\n      dtype=object)\n\n\n\n\n\nFindings on 8 repos\n\ndf_repo_run_file.repo.unique()\n\narray(['lightfm', 'qlib', 'mmf', 'nanodet', 'magenta', 'DeepSpeech',\n       'paperless-ng', 'mycroft-core', 'apollo'], dtype=object)\n\n\n\n1. Overview of accuracy and consistency lightfm evaluation\nLet the ground truth of the lightfm is as the following:\n\n# Ground truth\nground_truth = pd.DataFrame([\n    {'repo': 'lightfm', 'id': '2.1', 'score': 1},\n    {'repo': 'lightfm', 'id': '3.2', 'score': 1},\n    {'repo': 'lightfm', 'id': '3.5', 'score': 0},\n    {'repo': 'lightfm', 'id': '4.2', 'score': 1},\n    {'repo': 'lightfm', 'id': '5.3', 'score': 0.5},\n    {'repo': 'lightfm', 'id': '6.1', 'score': 1},\n    {'repo': 'lightfm', 'id': '6.2', 'score': 1},\n    {'repo': 'qlib', 'id': '2.1', 'score': 0.5},\n    {'repo': 'qlib', 'id': '3.2', 'score': 1},\n    {'repo': 'qlib', 'id': '3.5', 'score': 0},\n    {'repo': 'qlib', 'id': '4.2', 'score': 0.5},\n    {'repo': 'qlib', 'id': '5.3', 'score': 1},\n    {'repo': 'qlib', 'id': '6.1', 'score': 1},\n    {'repo': 'qlib', 'id': '6.2', 'score': 1},\n    {'repo': 'DeepSpeech', 'id': '2.1', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '3.2', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '3.5', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '4.2', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '5.3', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '6.1', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '6.2', 'score': 0},\n])\nground_truth[ground_truth.repo == 'lightfm']\n\n\n\n\n\n\n\n\nrepo\nid\nscore\n\n\n\n\n0\nlightfm\n2.1\n1.0\n\n\n1\nlightfm\n3.2\n1.0\n\n\n2\nlightfm\n3.5\n0.0\n\n\n3\nlightfm\n4.2\n1.0\n\n\n4\nlightfm\n5.3\n0.5\n\n\n5\nlightfm\n6.1\n1.0\n\n\n6\nlightfm\n6.2\n1.0\n\n\n\n\n\n\n\n\ngenerate_stat_plot(df_repo__stat, ground_truth=ground_truth, repo=\"lightfm\", facet_col='repo')\n\n\n\n\n\n\n\nThe distribution of the scores for each checklist items:\n\ndf_repo__count.query('repo == \"lightfm\"')\n\n\n\n\n\n\n\n\nrepo\nlevel_1\n2.1\n3.2\n3.5\n4.2\n5.3\n6.1\n6.2\n\n\n\n\n6\nlightfm\n0.0\n0.0\n1.0\n19.0\n0.0\n18.0\n0.0\n0.0\n\n\n7\nlightfm\n0.5\n1.0\n29.0\n6.0\n27.0\n12.0\n20.0\n4.0\n\n\n8\nlightfm\n1.0\n29.0\n0.0\n5.0\n3.0\n0.0\n10.0\n26.0\n\n\n\n\n\n\n\nObservations: The system evaluation kind of aligns with our evaluation, that is, - for those items that we believe “Satisfied” (Score = 1), the system mostly output 0.5 or 1 - for those items that we believe “Partially Satisfied” or “Not Satisfied”, the system mostly output 0.5 or 0 - some checklist items display high variance, e.g. 3.5, 5.3 and 6.1.\n\n\n2. Overview of qlib\nLet the ground truth of the qlib is as the following (FIXME: to be confirmed):\n\n# Ground truth\nground_truth[ground_truth.repo == 'qlib']\n\n\n\n\n\n\n\n\nrepo\nid\nscore\n\n\n\n\n7\nqlib\n2.1\n0.5\n\n\n8\nqlib\n3.2\n1.0\n\n\n9\nqlib\n3.5\n0.0\n\n\n10\nqlib\n4.2\n0.5\n\n\n11\nqlib\n5.3\n1.0\n\n\n12\nqlib\n6.1\n1.0\n\n\n13\nqlib\n6.2\n1.0\n\n\n\n\n\n\n\n\ngenerate_stat_plot(df_repo__stat, ground_truth=ground_truth, repo=\"qlib\", facet_col='repo')\n\n\n\n\n\n\n\n\ndf_repo__count.query('repo == \"qlib\"')\n\n\n\n\n\n\n\n\nrepo\nlevel_1\n2.1\n3.2\n3.5\n4.2\n5.3\n6.1\n6.2\n\n\n\n\n24\nqlib\n0.0\n0.0\n1.0\n29.0\n3.0\n14.0\n4.0\n1.0\n\n\n25\nqlib\n0.5\n0.0\n12.0\n1.0\n27.0\n16.0\n24.0\n26.0\n\n\n26\nqlib\n1.0\n30.0\n17.0\n0.0\n0.0\n0.0\n2.0\n3.0\n\n\n\n\n\n\n\nObservations: - There are more disagreement between system and manual evaluation - especially for 5.3, 6.1, 6.2. - The items consistency in this repo are not similar to those in lightfm. - e.g. Variance for 3.5 is greatly reduced. Variance for 3.2 becomes larger. - However, qlib is not just a machine learning project, it also contains a software inside. - e.g. It has a lot of randomly generated data by itself, instead of reading a data to perform analysis, it seems to deviate from the objective of 2.1.\n\n\n3. The consistency for each checklist items\n\nWhy is it important? If the score of a particular item varies a lot when evaluating a repository, it might mean that its prompt (Requirement) is confusing to the LLM, or the checklist item itself is not well defined.\n\n\ndf_repo__stat.pivot(index='id', columns='repo', values='std')\n\n\n\n\n\n\n\nrepo\nDeepSpeech\napollo\nlightfm\nmagenta\nmmf\nmycroft-core\nnanodet\npaperless-ng\nqlib\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1\n0.479463\n0.152564\n0.091287\n0.423451\n0.000000\n0.000000\n0.000000\n0.242117\n0.000000\n\n\n3.2\n0.406838\n0.215092\n0.091287\n0.189525\n0.245066\n0.278027\n0.239732\n0.091287\n0.285673\n\n\n3.5\n0.000000\n0.000000\n0.388040\n0.252003\n0.126854\n0.000000\n0.252003\n0.000000\n0.091287\n\n\n4.2\n0.000000\n0.000000\n0.152564\n0.091287\n0.126854\n0.000000\n0.254274\n0.000000\n0.152564\n\n\n5.3\n0.000000\n0.000000\n0.249136\n0.000000\n0.126854\n0.000000\n0.000000\n0.000000\n0.253708\n\n\n6.1\n0.351107\n0.172873\n0.239732\n0.252003\n0.233046\n0.000000\n0.285673\n0.000000\n0.224888\n\n\n6.2\n0.000000\n0.000000\n0.172873\n0.000000\n0.201289\n0.253708\n0.260415\n0.126854\n0.182574\n\n\n\n\n\n\n\n\nalt.Chart(df_repo__stat).mark_boxplot().encode(\n    x=\"std:Q\",\n    y='id:N'\n).properties(\n    height=200,\n    width=400\n)\n\n\n\n\n\n\n\nObservations: - The evaluation of the checklist item 2.1 Ensure Data File Loads as Expected is usually stable. - When evaluating a repository, 50% of the time its standard deviation is smaller than 0.05, the smallest among the others.\nBelow shows the breakdown of item scores for each repository:\n(NOTE: only lightfm and qlib have ground truth, in green diamond)\n\ngenerate_stat_plot(df_repo__stat, ground_truth=ground_truth, facet_col='id')\n\n\n\n\n\n\n\nObservations: - (TBC) The standard deviation for Item 3.5 and 5.3 shows great variation, which might imply that test cases in some repo might be confusing to LLM while some are clear. - (TBC) The standard deviation for Item 5.3, 6.1, 6.2 are relatively high and consistent, which might imply that there is room for refining the prompt to reduce consistency issue.\n\n\n4. The consistency for each checklist items, compared to the lightfm\n\nWhy is it important? We optimized the consistency of our system using lightfm. Therefore, we treat this repository as a benchmark. If a particular checklist item has a much worse consistency in other repository, that might mean that the prompt for that item is not generalizable.\n\nBelow shows the standard deviations in a 30 runs for each checklist item for each repository:\n\nstds = df_repo__stat[['repo', 'std', 'id']].pivot(index='repo', columns='id')\nstds\n\n\n\n\n\n\n\n\nstd\n\n\nid\n2.1\n3.2\n3.5\n4.2\n5.3\n6.1\n6.2\n\n\nrepo\n\n\n\n\n\n\n\n\n\n\n\nDeepSpeech\n0.479463\n0.406838\n0.000000\n0.000000\n0.000000\n0.351107\n0.000000\n\n\napollo\n0.152564\n0.215092\n0.000000\n0.000000\n0.000000\n0.172873\n0.000000\n\n\nlightfm\n0.091287\n0.091287\n0.388040\n0.152564\n0.249136\n0.239732\n0.172873\n\n\nmagenta\n0.423451\n0.189525\n0.252003\n0.091287\n0.000000\n0.252003\n0.000000\n\n\nmmf\n0.000000\n0.245066\n0.126854\n0.126854\n0.126854\n0.233046\n0.201289\n\n\nmycroft-core\n0.000000\n0.278027\n0.000000\n0.000000\n0.000000\n0.000000\n0.253708\n\n\nnanodet\n0.000000\n0.239732\n0.252003\n0.254274\n0.000000\n0.285673\n0.260415\n\n\npaperless-ng\n0.242117\n0.091287\n0.000000\n0.000000\n0.000000\n0.000000\n0.126854\n\n\nqlib\n0.000000\n0.285673\n0.091287\n0.152564\n0.253708\n0.224888\n0.182574\n\n\n\n\n\n\n\n\nstds_p = stds.copy()\nstds_p.columns = [col[1] for col in stds_p.columns]\nstds_p = stds_p.reset_index()\nstds_p = stds_p.melt(id_vars='repo', var_name='id')\n\n\nstds_p.head()\n\n\n\n\n\n\n\n\nrepo\nid\nvalue\n\n\n\n\n0\nDeepSpeech\n2.1\n0.479463\n\n\n1\napollo\n2.1\n0.152564\n\n\n2\nlightfm\n2.1\n0.091287\n\n\n3\nmagenta\n2.1\n0.423451\n\n\n4\nmmf\n2.1\n0.000000\n\n\n\n\n\n\n\n\n# stripplot = (\n#     alt.Chart(stds_p)\n#     .mark_point(filled=True, size=100)\n#     .transform_calculate( \n#         # Generate Gaussian jitter with a Box-Muller transform \n#         jitter='sqrt(-2*log(random()))*cos(2*PI*random())'\n#         # jitter='random()'\n#     ).encode( \n#         y=alt.Y( \n#             'jitter:Q', \n#             title=None, \n#             axis=alt.Axis(ticks=False, grid=True, labels=False), \n#             scale=alt.Scale(), \n#         ), \n#         x=alt.X('value:Q'), \n#         color=alt.Color('repo:N'),\n#         row=alt.Row( \n#             'id:N',\n#             header=alt.Header(\n#                 labelFontSize=16,\n#                 labelAngle=0\n#             )\n#         ),\n#         tooltip='repo'\n#     ).configure_facet( \n#         spacing=0\n#     ).configure_view( \n#         stroke=None\n#     ).configure_axis( \n#         labelFontSize=16, \n#         titleFontSize=16\n#     ).properties(\n#         height=50, \n#         width=600\n#     ) \n# )\n    \n# stripplot \n\n\ndef generate_jitterbox_plot(df_stds_p):\n    \"\"\"\n    Generate jitterbox plot across all repo and all checklist item\n    \"\"\"\n    box = alt.Chart().mark_boxplot(\n        color='grey',\n        opacity=0.5,\n        size=20,\n    ).encode(\n        x=alt.X('value:Q').title('SD(Score)'),\n        y=alt.Y('id:N', title=None, axis=alt.Axis(labelPadding=10, grid=False))\n    )\n    \n    stripplot = alt.Chart().mark_circle(size=100).encode(\n        y=alt.Y( \n            'id:N',\n            axis=alt.Axis(ticks=False, grid=True, labels=True), \n            scale=alt.Scale(), \n        ), \n        x='value:Q',\n        yOffset=\"jitter:Q\",\n        color=alt.Color('id:N', legend=None),\n        tooltip='repo'\n    ).transform_calculate(\n        # Generate Gaussian jitter with a Box-Muller transform\n        jitter=\"sqrt(-2*log(random()))*cos(2*PI*random())\"\n    )\n    \n    plot = alt.layer(\n        box,\n        stripplot,\n        data=df_stds_p\n    ).configure_view( \n        stroke=None\n    ).configure_axis( \n        labelFontSize=16, \n        titleFontSize=16\n    ).properties(\n        height=300, \n        width=600\n    ) \n    \n    return plot\n\n\ngenerate_jitterbox_plot(stds_p)\n\n\n\n\n\n\n\n\nalt.Chart(df_repo__stat).mark_boxplot().encode(\n    x=\"std:Q\",\n    y='id:N'\n).properties(\n    height=200,\n    width=400\n)\n\n\n\n\n\n\n\n\n# !pip install altair_catplot\n# !pip install seaborn\n\n\n# import altair_catplot\n\n# altair_catplot.catplot(\n#     stds_p, \n#     transform ='jitterbox', \n#     mark ='point', \n#     encoding = dict(\n#         x = alt.X('value:Q'), \n#         y = alt.Y('id:N'), \n#         color = alt.Color('repo:N')\n#     ) \n# )\n\n\nF = stds.drop(index='lightfm') / stds.loc['lightfm']\n\nbase = alt.Chart(\n    F.melt(ignore_index=False).reset_index()[['repo', 'id', 'value']]\n).transform_calculate(\n    benchmark=\"1\",\n    threshold=f\"{scipy.stats.f.ppf(0.975, 29, 29)}\"\n)\n\npoint = base.mark_point(\n    filled=True,\n    size=100,\n).encode(\n    x=alt.X('value:Q').title(\"std ratio (c.f. lightfm)\"),\n    y='id:N',\n    color='repo',\n    tooltip='repo'\n).properties(\n    height=200,\n    width=400\n)\n\npoint \\\n+ base.mark_rule(color='black').encode(x=\"benchmark:Q\") \\\n+ base.mark_rule(color='red').encode(x=\"threshold:Q\")\n# jitter instead of mark_point &lt;-- prompt vs. repo problem?\n# prompt: sd of checklist item for all repo is high\n# repo: most of repo have low sd, the repo we're looking at has outlier\n\n\n\n\n\n\n\nObservations: - The evaluation of the checklist item 3.2 Data in the Expected Format becomes much more unstable in most of other repositories. - That of the 2.1 is significantly unstable in the repo paperless-ng, magenta and DeepSpeech, but it may be due to the repo itself.\nTODO: to look into the 3.2’s scores.\n\n\nTODO: Given ground truth == 1, distribution of system score?\n\n\nTODO: Given ground truth == 0, distribution of system score?\n\ndef generate_histogram_plot(df_repo_run_long, df_ground_truth=None, repo=None, id=None):\n    \"\"\"\n    Generate histogram across all repo and all checklist item\n    Optional to incorporate ground truth and select specific repo/checklist item\n    \"\"\"\n    # data\n    repo_data = df_repo_run_long.copy()\n    if repo:\n        repo_data = repo_data.query(f'repo == \"{repo}\"')\n    if id:\n        repo_data = repo_data.query(f'id == \"{id}\"')\n\n    # base histogram chart\n    base = alt.Chart().mark_bar().encode(\n                x=alt.X('eval_score:Q', title='Score'), \n                y=alt.Y('count()'), \n                color=alt.value('grey'),\n                size=alt.value(20),\n            )\n    \n    if df_ground_truth is not None:\n        # data\n        gt_data = df_ground_truth.copy()\n        if repo:\n            gt_data = gt_data.query(f'repo == \"{repo}\"')\n        if id:\n            gt_data = gt_data.query(f'id == \"{id}\"')\n        \n        repo_data = pd.merge(repo_data, gt_data, how='left', on=['repo', 'id'])\n        repo_data['is_equal_to_gt'] = repo_data['eval_score'] == repo_data['score']\n        \n        # base histogram chart\n        base = base.encode(\n                    color=alt.Color('is_equal_to_gt', scale=alt.Scale(range=['grey', 'green']), legend=None)\n                )\n        base += base.mark_text().encode(\n            text=alt.value('Ground Truth'),\n            x='score',\n            size=alt.value(10),\n            color=alt.value('green'),\n        )\n\n    plot = alt.layer(\n                base,\n                data=repo_data\n            ).properties(\n                width=200,\n                height=200,\n            ).facet(\n                row='repo',\n                column='id'\n            )        \n    \n    return plot\n\n\n\nContingency Table\n\ndf_repo_run_p = pd.melt(df_repo_run, id_vars=['repo', 'run'], var_name='id', value_name='eval_score')\ndf_repo_run_p = pd.merge(df_repo_run_p, ground_truth, how='inner', on=['repo', 'id'])\ndf_repo_run_p = df_repo_run_p.rename(columns={'score': 'ground_truth'})\npd.pivot_table(df_repo_run_p, values='run', index=['ground_truth'], columns=['eval_score'], aggfunc='count', fill_value=0)\n\n\n\n\n\n\n\neval_score\n0.0\n0.5\n1.0\n\n\nground_truth\n\n\n\n\n\n\n\n0.0\n227\n8\n35\n\n\n0.5\n21\n39\n30\n\n\n1.0\n21\n159\n90\n\n\n\n\n\n\n\n\ndf_repo_run_p\n\n\n\n\n\n\n\n\nrepo\nrun\nid\neval_score\nground_truth\n\n\n\n\n0\nDeepSpeech\n1\n2.1\n1.0\n0.0\n\n\n1\nDeepSpeech\n2\n2.1\n1.0\n0.0\n\n\n2\nDeepSpeech\n3\n2.1\n0.0\n0.0\n\n\n3\nDeepSpeech\n4\n2.1\n1.0\n0.0\n\n\n4\nDeepSpeech\n5\n2.1\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n625\nqlib\n26\n6.2\n0.5\n1.0\n\n\n626\nqlib\n27\n6.2\n0.5\n1.0\n\n\n627\nqlib\n28\n6.2\n0.5\n1.0\n\n\n628\nqlib\n29\n6.2\n1.0\n1.0\n\n\n629\nqlib\n30\n6.2\n0.5\n1.0\n\n\n\n\n630 rows × 5 columns\n\n\n\n\n# generate_histogram_plot(df_repo_run_p, df_ground_truth=ground_truth)"
  },
  {
    "objectID": "04_plots-for-presentations.html",
    "href": "04_plots-for-presentations.html",
    "title": "Accuracy: Contingency table",
    "section": "",
    "text": "!pip install scipy altair\n\nRequirement already satisfied: scipy in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (1.13.1)\nCollecting altair\n  Using cached altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: numpy&lt;2.3,&gt;=1.22.4 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from scipy) (1.26.4)\nRequirement already satisfied: jinja2 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from altair) (3.1.4)\nRequirement already satisfied: jsonschema&gt;=3.0 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from altair) (4.22.0)\nRequirement already satisfied: packaging in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from altair) (23.2)\nRequirement already satisfied: pandas&gt;=0.25 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from altair) (2.2.2)\nCollecting toolz (from altair)\n  Using cached toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: attrs&gt;=22.2.0 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from jsonschema&gt;=3.0-&gt;altair) (23.2.0)\nRequirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from jsonschema&gt;=3.0-&gt;altair) (2023.12.1)\nRequirement already satisfied: referencing&gt;=0.28.4 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from jsonschema&gt;=3.0-&gt;altair) (0.35.1)\nRequirement already satisfied: rpds-py&gt;=0.7.1 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from jsonschema&gt;=3.0-&gt;altair) (0.18.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from pandas&gt;=0.25-&gt;altair) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from pandas&gt;=0.25-&gt;altair) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from pandas&gt;=0.25-&gt;altair) (2024.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from jinja2-&gt;altair) (2.1.5)\nRequirement already satisfied: six&gt;=1.5 in /Users/johnshiu/miniconda3/envs/test-creation/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.25-&gt;altair) (1.16.0)\nUsing cached altair-5.3.0-py3-none-any.whl (857 kB)\nUsing cached toolz-0.12.1-py3-none-any.whl (56 kB)\nInstalling collected packages: toolz, altair\nSuccessfully installed altair-5.3.0 toolz-0.12.1\n\n\n\nimport scipy\nimport pickle\nimport json\nimport yaml\nimport pandas as pd\nimport altair as alt\nfrom collections import Counter\n\n\ndef get_report(response):\n    report = []\n    for result in response.call_results:\n        if result.parsed_response:\n            resp = result.parsed_response['results']\n            for item in resp:\n                item['file'] = result.files_evaluated[0] \n                item['success'] = result.success\n                report.append(item)\n        else:\n            report.append({\n                'ID': '2.1', # FIXME\n                'Title': '',\n                'Requirement': '',\n                'Observation': '',\n                'Functions': [],\n                'Evaluation': '',\n                'Score': 0,\n                'file': result.files_evaluated[0],\n                'success': result.success\n            })\n    return pd.DataFrame(report)\n\ndef get_report_json(response):\n    report = []\n    for result in response['call_results']:\n        if result['parsed_response']:\n            resp = result['parsed_response']['results']\n            for item in resp:\n                item['file'] = result['files_evaluated'][0] \n                item['success'] = result['success']\n                report.append(item)\n        else:\n            report.append({\n                'ID': '2.1', # FIXME\n                'Title': '',\n                'Requirement': '',\n                'Observation': '',\n                'Functions': [],\n                'Evaluation': '',\n                'Score': 0,\n                'file': result.files_evaluated[0],\n                'success': result.success\n            })\n    return pd.DataFrame(report)\n\ndef extract_file_and_scores(resp_path, verbose=False):\n    if verbose:\n        print(resp_path)\n    with open(resp_path, 'rb') as file:\n        try:\n            response = pickle.load(file)\n            report = get_report(response)\n        except:\n            response = json.load(file)\n            report = get_report_json(response)\n    df = (\n        report\n        .pivot(index='file', columns='ID', values='Score')\n        .rename_axis(None, axis=1)\n    )\n    df['success'] = report.groupby(['file'])['success'].all()\n    df['response_path'] = resp_path\n    return df.reset_index()\n\n\nchecklist_ids = ['2.1', '3.2', '3.5', '4.2', '5.3', '6.1', '6.2']\n\ndef read_and_preprocess(result_path):\n    with open(result_path, 'r') as file:\n        config = pd.DataFrame(yaml.safe_load(file))\n    \n    # prepare score data by repo, run, file\n    tmp = [\n        extract_file_and_scores(path) for path in config['response_path'] # FIXME: excluded deepchem\n    ]\n    tmp = pd.concat(tmp, axis=0).reset_index(drop=True)\n    \n    raw_df_repo_run_file = config.merge(tmp, on='response_path', how='left')\n\n    # filter non-test files in qlib\n    df_repo_run_file = raw_df_repo_run_file.query('(repo != \"qlib\") | (file.str.contains(\"../data/raw/openja/qlib/tests/\"))')\n    \n    # prepare score data by repo, run\n    df_repo_run = df_repo_run_file.groupby(['repo', 'run']).agg({\n        id: ['max'] for id in checklist_ids\n    })\n    df_repo_run.columns = [col[0] for col in df_repo_run.columns]\n    df_repo_run = df_repo_run.reset_index()\n    \n    # prepare statistics of scores by repo\n    df_repo__stat = df_repo_run.groupby(['repo']).agg({\n        id: ['mean', 'std', 'count'] for id in checklist_ids\n    })\n    df_repo__stat = pd.melt(df_repo__stat.reset_index(), id_vars=[('repo', '')])\n    df_repo__stat.columns = ['repo', 'id', 'stat', 'value']\n    df_repo__stat = (\n        df_repo__stat.pivot(index=['repo', 'id'], columns='stat', values='value')\n        .reset_index()\n        .rename_axis(None, axis=1)\n    )\n    \n    # prepare counting of scores by repo\n    df_repo__count = df_repo_run.groupby(['repo'])['2.1'].apply(Counter).reset_index()\n    for id in checklist_ids[1:]:\n        df_repo__count = df_repo__count.merge(\n            df_repo_run.groupby(['repo'])[id].apply(Counter).reset_index(),\n            on=['repo', 'level_1'],\n            how='outer'\n        )\n    \n    df_repo__count = df_repo__count.fillna(0)\n\n    return (df_repo_run_file, df_repo_run, df_repo__stat, df_repo__count)\n\n\n# Ground truth\nground_truth = pd.DataFrame([\n    {'repo': 'lightfm', 'id': '2.1', 'score': 1},\n    {'repo': 'lightfm', 'id': '3.2', 'score': 1},\n    {'repo': 'lightfm', 'id': '3.5', 'score': 0},\n    {'repo': 'lightfm', 'id': '4.2', 'score': 1},\n    {'repo': 'lightfm', 'id': '5.3', 'score': 0.5},\n    {'repo': 'lightfm', 'id': '6.1', 'score': 1},\n    {'repo': 'lightfm', 'id': '6.2', 'score': 1},\n    {'repo': 'qlib', 'id': '2.1', 'score': 0.5},\n    {'repo': 'qlib', 'id': '3.2', 'score': 1},\n    {'repo': 'qlib', 'id': '3.5', 'score': 0},\n    {'repo': 'qlib', 'id': '4.2', 'score': 0.5},\n    {'repo': 'qlib', 'id': '5.3', 'score': 1},\n    {'repo': 'qlib', 'id': '6.1', 'score': 1},\n    {'repo': 'qlib', 'id': '6.2', 'score': 1},\n    {'repo': 'DeepSpeech', 'id': '2.1', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '3.2', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '3.5', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '4.2', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '5.3', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '6.1', 'score': 0},\n    {'repo': 'DeepSpeech', 'id': '6.2', 'score': 0},\n])\n\n\nid_item_map = {\n    '2.1': 'Ensure Data File Loads as Expected',\n    '3.2': 'Data in the Expected Format',\n    '3.5': 'Check for Duplicate Records in Data',\n    '4.2': 'Verify Data Split Proportion',\n    '5.3': 'Ensure Model Output Shape Aligns with Expectation',\n    '6.1': 'Verify Evaluation Metrics Implementation',\n    '6.2': \"Evaluate Model's Performance Against Thresholds\"\n}\n\n\n#result_path = '../draft/batch_run_results/record_combine.yml'\ndf_repo_run_file, df_repo_run, df_repo__stat, df_repo__count = read_and_preprocess(\n    '../data/processed/batch_run/record_combine.yml'\n)\n\n\ncont_table = pd.melt(\n    df_repo_run.query('(repo == \"lightfm\")')[['repo', 'run', '3.5', '4.2', '5.3']], \n    id_vars=['repo', 'run'], var_name='id', value_name='System Output')\ncont_table = pd.merge(cont_table, ground_truth, how='inner', on=['repo', 'id'])\ncont_table = cont_table.rename(columns={'score': 'ground_truth'})\ncont_table['title'] = cont_table['id'].apply(lambda x: id_item_map[x])\n#cont_table = cont_table[['repo', 'title', 'ground_truth', 'System Output', 'run']]\ncont_table = pd.pivot_table(cont_table, values='run', index=['repo', 'id', 'title', 'ground_truth'], columns=['System Output'], aggfunc='count', fill_value=0)\ncont_table.index.names = ['Repository', 'ID', 'Title', 'Ground Truth']\ncont_table.sort_index(level=3)\n\n\n\n\n\n\n\n\n\n\nSystem Output\n0.0\n0.5\n1.0\n\n\nRepository\nID\nTitle\nGround Truth\n\n\n\n\n\n\n\nlightfm\n3.5\nCheck for Duplicate Records in Data\n0.0\n19\n6\n5\n\n\n5.3\nEnsure Model Output Shape Aligns with Expectation\n0.5\n18\n12\n0\n\n\n4.2\nVerify Data Split Proportion\n1.0\n0\n27\n3\n\n\n\n\n\n\n\n\nConsistency: jitterbox plot\n\nstds = df_repo__stat[['repo', 'std', 'id']].pivot(index='repo', columns='id').copy()\nstds.columns = [col[1] for col in stds.columns]\nstds = stds.reset_index()\nstds = stds.melt(id_vars='repo', var_name='id')\nstds['title'] = stds['id'].apply(lambda x: id_item_map[x])\n\n\nbox = alt.Chart().mark_boxplot(\n    color='grey',\n    opacity=0.5,\n    size=20,\n).encode(\n    x=alt.X('value:Q').title('System Output Uncertainty'),\n    y=alt.Y('title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))\n)\n\nstripplot = alt.Chart().mark_circle(size=100).encode(\n    y=alt.Y( \n        'title:N',\n        axis=alt.Axis(ticks=False, grid=True, labels=True), \n        scale=alt.Scale(), \n    ), \n    x='value:Q',\n    yOffset=\"jitter:Q\",\n    color=alt.Color('id:N', legend=None),\n    tooltip='repo'\n).transform_calculate(\n    # Generate Gaussian jitter with a Box-Muller transform\n    jitter=\"sqrt(-2*log(random()))*cos(2*PI*random())\"\n)\n\nplot = alt.layer(\n    box,\n    stripplot,\n    data=stds\n).configure_view( \n    stroke=None\n).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n).properties(\n    height=300, \n    width=600,\n    title=\"30 Runs on Openja's Repositories for each Checklist Item\"\n) \n\n\nplot\n\n\n\n\n\n\n\n\n\nimprovement from 3.5 to 4o\n\n#result_path = '../draft/batch_run_results/record_combine.yml'\ndf_repo_run_file_4o, df_repo_run_4o, df_repo_4o__stat, df_repo_4o__count = read_and_preprocess(\n    '../data/processed/batch_run_4o/record_combine.yml'\n)\n\n\ndf_repo_4o__stat\n\n\n\n\n\n\n\n\nrepo\nid\ncount\nmean\nstd\n\n\n\n\n0\nlightfm\n2.1\n30.0\n1.0\n0.0\n\n\n1\nlightfm\n3.2\n30.0\n1.0\n0.0\n\n\n2\nlightfm\n3.5\n30.0\n1.0\n0.0\n\n\n3\nlightfm\n4.2\n30.0\n1.0\n0.0\n\n\n4\nlightfm\n5.3\n30.0\n1.0\n0.0\n\n\n5\nlightfm\n6.1\n30.0\n1.0\n0.0\n\n\n6\nlightfm\n6.2\n30.0\n1.0\n0.0\n\n\n\n\n\n\n\n\ndf1 = df_repo__stat.query('(repo == \"lightfm\") & (id == \"4.2\")').copy()\ndf1['model'] = ['gpt-3.5-turbo']\n\ndf2 = df_repo_4o__stat.query('(repo == \"lightfm\") & (id == \"4.2\")').copy()\ndf2['model'] = ['gpt-4o']\n\ndf_model_comp = pd.concat((df1, df2), axis=0)\n\n\nbase = alt.Chart(df_model_comp).transform_calculate(\n    min=\"max(0, datum.mean-datum.std)\",\n    max=\"min(1, datum.mean+datum.std)\"\n)\n    \n# generate the points\npoints = base.mark_point(\n    filled=True,\n    size=50,\n    color='black'\n).encode(\n    x=alt.X('mean:Q').scale(domainMin=0, domainMax=1).title(\"System Output\").axis(\n        labelExpr=\"datum.value % 0.5 ? null : datum.label\"\n    ),\n    y=alt.Y('model:N').title(\"Model\")#.scale(domainMin=0, domainMax=1).title('Score'),\n)\n    \n# generate the error bars\nerrorbars = base.mark_errorbar().encode(\n    x=alt.X(\"min:Q\").title('1 SD'), #\"id:N\",\n    x2=\"max:Q\",\n    y=\"model:N\"\n)\n\n(points + errorbars).configure_axis( \n    labelFontSize=12, \n    titleFontSize=12\n).properties(\n    height=200,\n    width=400,\n    title={\n        'text': '30 Runs on Checklist Item: \"Ensure Data File Loads as Expected\"',\n        'subtitle': \"Ground Truth = 1\"\n    }\n)"
  },
  {
    "objectID": "final_report.html",
    "href": "final_report.html",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "",
    "text": "by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin"
  },
  {
    "objectID": "final_report.html#executive-summary",
    "href": "final_report.html#executive-summary",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Executive Summary",
    "text": "Executive Summary\n#FIXME"
  },
  {
    "objectID": "final_report.html#introduction",
    "href": "final_report.html#introduction",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Introduction",
    "text": "Introduction\n\nProblem Statement\nThe global artificial intelligence (AI) market is growing exponentially {cite}grand2021artificial, driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis.\nHowever, ensuring the software quality of these systems remains a significant challenge {cite}openja2023studying. Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as substantial financial losses ({cite}Asheeta2019, {cite}Asheeta2019, {cite}Asheeta2019) and safety hazards.\nTherefore, defining and promoting an industry standard and establishing robust testing methodologies for these systems is crucial. But how?\n\n\nOur Objectives\nWe propose to develop testing suites diagnostic tools based on Large Language Models (LLMs) and curate checklists based on ML research papers and best practices to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance applied ML software’s trustworthiness, quality, and reproducibility across both the industry and academia {cite}kapoor2022leakage."
  },
  {
    "objectID": "final_report.html#data-science-methods",
    "href": "final_report.html#data-science-methods",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Data Science Methods",
    "text": "Data Science Methods\n\nCurrent Approaches\nTo ensure the reproducibility, trustworthiness and free-of-bias ML system, comprehensive assessment is essential. We have observed some traditional approaches in assessing the quality of ML systems, which contain different advantages and drawbacks as follows.\n\n1. Code Coverage\nCode coverage is a measure of the proportion of source code of a program executed when a particular test suite is run. It is widely used in software development domain as one of the measurements. It quantifies the test quality and is scalable given the short process time. However, it cannot provide the reasons and in which ML areas that the test suites fall short under the context of ML system development.\n\n\n2. Manual Evaluation\nManual evaluation involves human expert review at the source code, whom can take the business logic into considerations and find vulnerabilites. Manual evaluation usually delivers comments for improvement under specific development context, and it is still one of the most reliable methods in practice. However, the time cost is large and it is not scalable due to the scarcity of time and human expert. Different human expert might put emphasis on different ML test areas instead of a comprehensive and holistic review on the ML system test suites.\n\n\n\nOur Approach\nOur approach is to deliver an automated code review tool with the best practices of ML test suites embedded, which can be used by ML users to learn the best practices as well as to obtain a comprehensive evaluation on their ML system codes.\nTo come up with the best practices of ML test suites, ML research paper and recognized online resources are our data. Under the collaboration with our partner, we have researched industrial best practices (cite: Microsoft, Jordan) and published academic literature (cite: OpenJa) and consolidated the testing strategies of ML projects into a format which is easily legible and editable by human (researchers, ML engineers, etc.). The format is also machine-friendly that can be easily incorporated into the automated tool.\nTo develop our automated code review tool, GitHub repositories of ML projects are our data. We have collected 11 repositories studied in {cite}openja2023studying, where these projects include comprehensive test suites and are written in Python programming language, for our product development. Our tool is capable of understanding the test suites in these projects, comparing and contrasting the test suites with the embedded best practices, and delivering evaluations and suggestions to the current test suties.\nBy developing our approach, we expect that it can provide reliable test suites evaluation to multiple ML projects in a scalable manner. However, we acknowledged that the consolidation of best practices currently focused on a few high priority test areas due to time constraint, where we expect to expand in the future. The test evaluation results provided by our tool are yet as reliable as human evaluation, where we will quantify its performance using the success metrics below.\n\n\nSuccess Metrics\nTo properly assess the performance of our tool which leverages the capability of LLMs, we have researched and taken reference of the methods in {cite}alexander2023evaluating and defined the 2 success metrics: accuracy and consistency. With these metrics, our users (researchers, ML engineers, etc.) can assess the trustworthiness while obtaining the evaluation results from our tool.\n\nAccuracy of the Application vs Human Expert Judgement\n\nWe run our tool on the ML projects in {cite}openja2023studying to obtain the evaluation results (i.e. completeness score) per each ML test best practice item. We then manually assess the test suites of these ML projects using the same criteria as the ground truth data. Machine evaluation results are compared and contrasted with the ground truth data. Accuracy is defined as the number of matching results over total number of results.\n\nConsistency of the Application\n\nMultiple runs on each ML project are performed and the evaluation results per each ML test best practice item are obtained. Standard deviation of these results per ML projects are calculated as a measure of consistency."
  },
  {
    "objectID": "final_report.html#data-product-results",
    "href": "final_report.html#data-product-results",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Data Product & Results",
    "text": "Data Product & Results\n\nData Products\nOur solution offers both a curated checklist on robust ML testing, and a Python package that facilitates the use of LLMs in checklist-based evaluation on the robustness of users’ ML projects. The Python package is made publicly available for distribution on the Python Packaging Index (PyPI).\nThe justifications for creating these products are, on one hand, checklists have been shown to decrease errors in software systems and promote code submissions (cite: Gawande 2010, Pineau et al. (2021) from Tiffany PDF). Moreover, Python is chosen to be the programing language of our package given its prevalence in the ML landscape, its ubiquitous presence across different OSes and the existence of Python libraries for the integration with LLMs. This lowers the barrier to use and develop our package and provides better user experience.\n\nHow to use the product\nThere are two ways to make use of this package:\n\nAs a CLI tool. A runnable command fixml is provided by the package. Once installed, users can perform the codebase evaluation, test function specification generation and other relevant tasks by running subcommands under fixml in terminal environment.\nAs a high-level API. Alternatively, one can use the package to import all components necessary for performing the tasks as part of their own system. Documentations are provided in terms of docstrings.\n\nBy formating our product as a CLI tool and API, one (researchers, ML engineers, etc.) will find it user-friendly to interact with. Moreover, it is versatile to support various use cases, such as web application development, data science research, etc.\n\n\nSystem Design\n(To be revised) \nThe design principle of our package adheres to object-oriented design and SOLID principles, which is fully modular. One can easily switch between different prompts, models and checklists to use. This enables code reuse and promote users’ collaboration to extend its functionality.\nThere are five components in the system of our package:\n\nCode Analyzer This component extracts the information relevant to test suites from the input codebase, which is essential for injecting only the most relevant information to LLMs given its token limits.\nPrompt Templates This component stores the prompt template necessary for instructing LLM to behave and return responses in consistent and expected format. Few-shot learning is applied for the instruction.\nChecklist This component reads the curated checklist, which is stored in CSV format, as a dict with fixed schema for injection into prompt. Default checklist is also included inside the package for distribution.\nRunners This component involves the Evaluator module, which evaluates each file from the test suites using LLMs and outputs evaluation results, and Generator module, which generates test specifications. Both modules include validation and retry logics and record all relevant information in the responses.\nParsers This components parses the responses from Evaluator into evaluation reports in various formats (HTML, PDF) using Jinja template engine. Adhering to our design principle, this enables flexibility in creating customized report structure.\n\n\n\nChecklist Design\nThe package will incorporate a checklist (Fig. 1) which contains the best practices in testing ML pipeline and is curated manually based on ML researches and recognized online resources. Prompt engineering is applied to the checklist for better performance. This also helps combating the hallucination of LLMs ({cite}zhang2023sirens) during the evaluation of ML projects by prompting it to follow exactly the checklist.\nHere is an example of how the checklist would be structured:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nID\nThe Unique Identifier of the checklist item\n\n\nTopic\nThe Test Area of the checklist item\n\n\nTitle\nThe Title of the checklist item\n\n\nRequirement\nThe Prompt of the checklist item to be injected into LLMs for evaluation\n\n\nExplanations\nDetailed explanations of the checklist item for human understanding\n\n\nReference\nReferences of the checklist item, e.g. academic paper\n\n\nIs Evaluator Applicable\nWhether the checklist item is selected to be used during evaluation. 0 indicates No, 1 indicates Yes\n\n\n\n(To be revised) \n\n\nArtifacts\nThere are three artifacts after using our package:\n\nEvaluation Responses The artifact stores both the evaluation responses from LLMs and meta-data of the process in JSON format. This supports downstream tasks, such as report render, scientific research, etc.\n\n(To be revised) schema of the JSON saved & what kind of information is stored\n\nEvaluation Report The artifact stores the evaluation results of the ML projects in a structured format, which includes completeness score breakdown and corresponding detailed reasons.\n\n(To be revised) \n\nTest Specification Script The artifacts stores the test specification responses from LLMs in Python script format.\n\n(To be revised) \n\n\n\nEvaluation Results\nAs illustrated in Success Metrics, we ran 30 iterations on each of the repositories in {cite}openja2023studying and examined the breakdown of the ML Completeness Score to assessed the quality of evaluation determined by our tool. (FIXME: would it be better to show a table of the repos? like how the Openja does?)\n\nAccuracy\nFor accuracy, we targeted 3 of the repositories (lightfm (FIXME: link), qlib (FIXME: link), DeepSpeech (FIXME: link)) for human evaluation and compared the ground truth with the outputs from our tool.\n\n\nCode\n# FIXME: table: checklist id, title, (ground truth, (lightfm, qlib, DeepSpeech))\n\n\n\nCaption: Ground truth data on the 3 repositories\n\n\n\nCode\n# FIXME: jitter-mean-sd plot (checklist item vs. score) for each repo\n\n\n\nCaption: Comparison of the satisfaction determined by our system versus the ground truth for each checklist item and repository\n\nWe found that our tool tends to undermine the actual satisfying cases. For the items that are actually satisfied (score = 1), our tool tends to classify as partially satisfied (score = 0.5), while for those that are partially satisfied (score = 0.5), our tool often classfies as not satisfied (score = 0).\n\n\nCode\n# FIXME: contingency table\n\n\n\nContingency table of the satisfaction determined by our system versus the ground truth\n\nThe accuracy issue may be attributed to the need for improvement of prompts in our checklist.\n\n\nConsistency\nSince the completeness score from LLMs contain randomness, we further studied the consistency of scores across checklist items and reposities.\n\n\nCode\n# FIXME: jitter-boxplot, checklist item vs. SD\n\n\n\nCaption: Standard deviations of the score for each checklist item. Each dot represents the standard deviation of scores of 30 runs of a sigle repository\n\nWe found 2 diverging cases. For example, it shows high standard deviations across repositories for item 3.2 Data in the Expected Format. This might be a proof of poor prompt quality, making it ambiguous for the LLM and hence hard to produce consistent results. Prompt engineering might solve this problem.\nOn the other hand, there are outliers yielding exceptionally high standard deviations for item 5.3 Ensure Model Output Shape Aligns with Expectation. This may be because those repositories are unorthodox, and careful manual examination is required to achieve a more robust conclusion.\n\n\nComparison of gpt-3.5-turbo and gpt-4o\nTo examine if newer LLMs help in both metrics, we preliminarily compared system outputs from gpt-4o and gpt-3.5-turbo on the lightfm repository, we observed that the gpt-4o system consistently returned “Satisfied”, which deviates from the ground truth.\n\n\nCode\n# FIXME: jitter-mean-sd plot (checklist item vs. score) for each repo\n\n\n\nCaption: Comparison of the satisfaction using gpt-4o versus using gpt-3.5-turbo for each checklist item on lightfm\n\nFurther investigation into gpt-4o is required to address this issue and enhance the system performance."
  },
  {
    "objectID": "final_report.html#conclusion",
    "href": "final_report.html#conclusion",
    "title": "Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Conclusion",
    "text": "Conclusion\n\nWrap Up\nOur project, FixML, represents a significant step forward in the field of machine learning (ML) testing by providing curated checklists and automated tools that enhance the evaluation and creation of test suites for ML models. The development and implementation of FixML have been driven by both the need of better quality assurance in ML systems, and the current limitations of traditional testing methods on ML projects which are either too general without comprehensive clarification, or are too human-reliant.\nFixML seamlessly takes in the user’s ML codebase, identifies and extracted its existing test suites. Together with the curated checklist on ML testing, FixML leverages Large Language Models (LLMs) to assess the completeness of the test suites and output detailed evaluation reports with completeness scores and specific reasons. This assists users in understanding the performance of their current test suites with insights. Additionally, FixML can generate test function specifications corresponding to the curated checklist, helping users utilizing their test suites.\nIn return, FixML solution combines the scalability of automated testing with the reliability of expert evaluation. By automating the evaluation process, FixML significantly reduces the time and human effort required to assess the quality of ML test suites. This popularizes thorough and efficient quality assessment on ML projects.\n\n\nLimitation & Future Improvement\nWhile FixML provides substantial benefits, there are limitations and areas that aim to be addressed in future development:\n\nSpecialized Checklist\n\nThe current checklist is designed to be general and may not cover all specific requirements for different ML projects. Future development will focus on creating more specialized checklists for different domains and project types, allowing for more tailored evaluations. Since the format of the checklist is designed to allow users to easily expand, edit and select checklist items based on their specific use case, we welcome any collaboration with ML researchers on the creation of specalized checklists.\n\nEnhanced Test Evaluator\n\nOur current study unveils the varying accuracy and consistency issues on the evaluation results using OpenAI GPT models. Future improvements involves prompt enhancement with prompt engineering techniques and support for multiple LLMs for higher performance and flexibility of FixML test evaluator functionality. We also expect to deliver user guidelines in editing the prompts in our system, where ML developers can customize prompts for better performance and collaborate with us to embed them into the system.\n\nCustomized Test Specification\n\nFixML test specification generator currently produces general test function skeletons solely based on the curated checklist without the context of the specific ML projects. Future developments will involve the integration of the ML project codebase in the generation process to output customized test functions skeletons. This further lower the barrier of ML users in creating comprehensive test suites relevant to the projects.\n\nWorkflow Optimization #FIXME: have to review whether to include as it seems lower priority.\n\nThe current test evaluator and test specification generator are separate entities. This could be improved by embedding a workflow engine that allows the system to automatically take actions based on the LLM response. For instance, if the LLM response suggests that test suites are partially satisfied or non-satisfied, the system could automatically run the test generator to produce test function skeletons and then reevaluate them until they are satisfied or some threshold is met. This would create a more cohesive and efficient workflow, reducing manual intervention and improving overall system performance.\n\nPerformance Optimization #FIXME: have to review whether to include as it seems lower priority.\n\nPerformance optimization is another critical area for future development. As FixML handles large codebases and complex evaluations, optimizing the system to handle these tasks more efficiently is essential. This includes improving the speed and accuracy of the LLM responses, reducing the time taken to analyze and generate reports, and ensuring the system can scale effectively to handle more extensive and more complex projects.\nBy addressing these limitations and focusing on these future improvements, FixML will become an even more powerful tool for ensuring the quality and robustness of machine learning and data science projects."
  },
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "",
    "text": "by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin"
  },
  {
    "objectID": "proposal.html#executive-summary",
    "href": "proposal.html#executive-summary",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Executive Summary",
    "text": "Executive Summary\nThe rapid growth of global artificial intelligence (AI) markets presents opportunities and challenges. While AI systems have the potential to impact various aspects of human life, ensuring their software quality remains a significant concern. Current testing strategies for machine learning (ML) systems lack standardization and comprehensiveness, which poses risks to stakeholders, such as financial losses and safety hazards.\nOur proposal addresses this challenge by developing a manually curated checklist which contains best practices and recommendations in testing ML systems. Additionally, an end-to-end application incorporating the checklist and Large Language Model (LLM) will be developed to analyze given ML system source codes and provide test completeness evaluation, missing test recommendations, and test function specification generation. Our proposed solution will enable users to systematically assess, improve, and include tests tailored to their ML systems through a combination of human expertise codified within the checklist and parametric memory from LLMs.\nIn the following weeks, we will develop and refine our product through a swift and efficient iterative development approach, with the aim to deliver a rigorously tested and fully-documented system to our partners by the end of the project."
  },
  {
    "objectID": "proposal.html#introduction",
    "href": "proposal.html#introduction",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Introduction",
    "text": "Introduction\n\nProblem Statement\nThe global artificial intelligence (AI) market is growing exponentially {cite}grand2021artificial, driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis.\nHowever, ensuring the software quality of these systems remains a significant challenge {cite}openja2023studying. Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as substantial financial losses {cite}Asheeta2019 and safety hazards.\nTherefore, defining and promoting an industry standard and establishing robust testing methodologies for these systems is crucial. But how?\n\n\nOur Objectives\nWe propose to develop testing suites diagnostic tools based on Large Language Models (LLMs) and curate a checklist to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance applied ML software’s trustworthiness, quality, and reproducibility across both the industry and academia {cite}kapoor2022leakage."
  },
  {
    "objectID": "proposal.html#our-product",
    "href": "proposal.html#our-product",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Our Product",
    "text": "Our Product\nOur solution offers an end-to-end application for evaluating and enhancing the robustness of users’ ML systems.\n\n\n\n```tuqkogfb ../../img/proposed_system_overview.png\n\n\n\n\nname: overview-diagram\n\n\n\nMain components and workflow of the proposed system. The checklist would be written in YAML to maximize readability for both humans and machines. We hope this will encourage researchers/users to read, understand and modify the checklist items, while keeping the checklist closely integrated with other components in our system.\n\nOne big challenge in utilizing LLMs to reliably and consistently evaluate ML systems is their tendency to generate illogical and/or factually wrong information known as hallucination {cite}`zhang2023sirens`.\n\nTo combat this, the proposed system will incorporate a checklist ([Fig. 1](overview-diagram)) which would be curated manually and incorporate best practices in software testing and identified areas to be tested inside ML pipeline from human experts and past research.\n\nThis checklist will be our basis in evaluating the effectiveness and completeness of existing tests in a given codebase. Relevant information will be injected into a prompt template, which the LLMs would then be prompted to follow the checklist **exactly** during the evaluation.\n\nHere is an example of how the proposed checklist would be structured:\n\n\n```{toggle}\n```yaml\n%YAML 1.2\n---\nTitle: Checklist for Tests in Machine Learning Projects\nDescription: &gt;\n  This is a comprehensive checklist for evaluating the data and ML pipeline\n  based on identified testing strategies from experts in the field.\nTest Areas:\n  - Topic: General\n    Description: &gt;\n      The following items describe best practices for all tests to be\n      written.\n    Tests:\n      - Title: Write Descriptive Test Names\n        Requirement: &gt;\n          Every test function should have a clear, descriptive name\n        Explanation: &gt;\n          If out tests are narrow and sufficiently descriptive, the test\n          name itself may give us enough information to start debugging.\n          This also helps us to identify what is being tested inside the\n          function.\n        References:\n          - https://testing.googleblog.com/2014/10/testing-on-toilet-writing-descriptive.html\n          - https://testing.googleblog.com/2024/05/test-failures-should-be-actionable.html\n\n      - Title: Keep Tests Focused\n        Requirement: &gt;\n          Each test should only test one scenario, meaning that in each\n          test we should only use one set of mock data.\n        Explanation: &gt;\n          If we test multiple scenarios in a single test, it is hard to\n          idenitfy exactly what went wrong. Keeping one scenario in a\n          single test helps us to isolate problematic scenarios.\n        References:\n          - https://testing.googleblog.com/2018/06/testing-on-toilet-keep-tests-focused.html\n\n      - Title: Prefer Narrow Assertions in Unit Tests\n        Requirement: &gt;\n          The assertions inside the tests should be narrow, meaning that\n          when checking a complex object, any unrelated behavior should\n          not be tested - Assert on only relevant behaviors.\n        Explanation: &gt;\n          If we have overly wide assertions (such as depending on every\n          field of a complex output proto), the test may fail for many\n          unimportant reasons. False positives are the opposite of\n          actionable.\n        References:\n          - https://testing.googleblog.com/2024/04/prefer-narrow-assertions-in-unit-tests.html\n\n      - Title: Keep Cause and Effect Clear\n        Requirement: &gt;\n          The modifications and the assertions of an object's behavior\n          in a single test should not be far away from each other.\n        Explanation: &gt;\n          Refrain from using large global test data structures shared\n          across multiple unit tests. This will allow for clear\n          identification of each test's setup and the cause and effect.\n        References:\n          - https://testing.googleblog.com/2017/01/testing-on-toilet-keep-cause-and-effect.html\n\n  - Topic: Data Presence\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the presence of data.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Data Quality\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the quality of data.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Data Ingestion\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      if the data is ingestion properly.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Model Fitting\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the model fitting process.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Model Evaluation\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      the model evaluation process.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\n  - Topic: Artifact Testing\n    Description: &gt;\n      The following items describe tests that need to be done for testing\n      any artifacts that are created from the project.\n    Tests:\n      - Title: ...\n        Requirement: ...\n        Explanation: ...\n        References:\n          - ...\n\nEvaluation Artifacts\nThe end goal of our product is to generate the following three artifacts in relation to the evaluation of a given ML system codebase:\n\nML Test Completeness Score: The application utilizes LLMs and our curated checklist to analyze users’ ML system source code and returns a comprehensive score of the system’s test quality.\nMissing Test Recommendations: The application evaluates the adequacy of existing tests for users’ ML code and offers recommendations for additional, system-specific tests to enhance testing effectiveness.\nTest Function Specification Generation: Users select desired test recommendations and prompt the application to generate test function specifications and references. These are reliable starting points for users to enrich the ML system test suites.\n\n\n\nSuccess Metrics\nOur product’s success will depend on mutation testing of the test functions developed based on our application-generated specifications. The evaluation metric is the success rate of detecting the perturbations introduced to the ML project code.\nOur partners and stakeholders expect a significant improvement in the testing suites of their ML systems post-application usage. As a result, the testing suites will demonstrate high accuracy in detecting faults, ensuring consistency and high quality of ML projects during updates.\n\n\nData Science Approach\n\nData: GitHub Repositories\nIn this project, GitHub repositories are our data.\nTo develop our testing checklist, we will collect 11 repositories studied in {cite}openja2023studying. Additionally, we will collect 377 repositories identified in the study by {cite}wattanakriengkrai2022github for our product development.\nFor each repository, we are interested in the metadata and the ML modeling- and test-related source code. The metadata will be retrieved using the GitHub API, while the source code will be downloaded and filtered using our custom scripts. To ensure the relevance of the repositories to our study, we will apply the following criteria for filtering: 1. Repositories that are related to ML systems. 2. Repositories that include test cases. 3. Repositories whose development is written in the Python programming language.\n\n\nMethodologies\nOur data science methodology incorporates human expert evaluation and prompt engineering to assess and enhance the test quality of ML systems.\n\nHuman Expert Evaluation\nWe will begin by formulating a comprehensive checklist for evaluating the data and ML pipeline based on the established testing strategies outlined in {cite}openja2023studying as the foundational framework. Based on the formulated checklist, our team will manually assess the test quality within each repository data. We will refine the checklist to ensure applicability and robustness when testing general ML systems.\nPrompt Engineering\nWe will engineer the prompts for LLM to incorporate with the ML system code and the curated checklist and to serve various purposes across the three-stage process:\n\nPrompts to examine test cases within the ML system source codes and deliver test completeness scores.\nPrompts to compare and contrast the existing tests and the checklist and deliver recommendations.\nPrompts to generate system-specific test specifications based on user-selected testing recommendations {cite}schafer2023empirical\n\n\n\n\nIterative Development Approach\nWe begin by setting up a foundational framework based on the selected GitHub repositories and research on ML testing. The framework might not cover all ML systems or testing practices. Therefore, we adopt an iterative development approach by establishing an open and scalable framework to address these considerations. The application will be continuously refined based on contributors’ insights.\nUsers are encouraged to interpret the generated artifacts with a grain of salt and recognize the evolving nature of ML system testing practices."
  },
  {
    "objectID": "proposal.html#delivery-timeline",
    "href": "proposal.html#delivery-timeline",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "Delivery Timeline",
    "text": "Delivery Timeline\nOur team follows the timeline below for our product delivery and prioritizes close communication with our partners to ensure that our developments align closely with their expectations.\n\n\n\n\n\n\n\nTimeline\nMilestones\n\n\n\n\nWeek 1 (Apr 29 - May 3)\nPrepare and Present Initial Proposal. Scrape repository data.\n\n\nWeek 2 - 3 (May 6 - 17)\nDeliver Proposal. Deliver Draft of ML Pipeline Test Checklist. Develop Minimum Viable Product (Test Completeness Score, Missing Test Recommendation)\n\n\nWeek 4 - 5 (May 20 - May 31)\nUpdate Test Checklist. Develop Test Function Specification Generator.\n\n\nWeek 6 (Jun 3 - Jun 7)\nUpdate Test Checklist. Wrap Up Product.\n\n\nWeek 7 (Jun 10 - Jun 14)\nFinalize Test Checklist. Perform Product System Test. Present Final Product. Prepare Final Product Report.\n\n\nWeek 8 (Jun 17 - Jun 21)\nDeliver Final Product. Deliver Final Product Report."
  },
  {
    "objectID": "proposal.html#references",
    "href": "proposal.html#references",
    "title": "Proposal Report - Checklists and LLM prompts for efficient and effective test creation in data analysis",
    "section": "References",
    "text": "References"
  }
]