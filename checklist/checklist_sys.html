<h1 id="checklist-for-tests-in-machine-learning-projects">Checklist for
Tests in Machine Learning Projects</h1>
<p><strong>Description</strong>: This is a comprehensive checklist for
evaluating the data and ML pipeline based on identified testing
strategies from experts in the field.</p>
<h2 id="general">1 General</h2>
<p><strong>Description</strong>: The following items describe best
practices for all tests to be written.</p>
<h2 id="data-presence">2 Data Presence</h2>
<p><strong>Description</strong>: The following items describe tests that
need to be done for testing the presence of data. This area of tests
mainly concern whether the reading and saving operations are behaving as
expected, and any unexpected behavior would not be passed silently.</p>
<h3 id="test-data-fetching-and-file-reading">2.1 Test Data Fetching and
File Reading</h3>
<p><strong>Requirement</strong>: Verify that the data fetching API or
data file reading functionality works correctly. Ensure that proper
error handling is in place for scenarios such as missing files,
incorrect file formats, and network errors.</p>
<p><strong>Explanation</strong>: Ensure that the code responsible for
fetching or reading data can handle errors. This means if the file is
missing, the format is wrong, or there's a network issue, the system
should not crash but should provide a clear error message indicating the
problem.</p>
<p><strong>References:</strong></p>
<ul>
<li>(general knowledge)</li>
</ul>
<h2 id="data-quality">3 Data Quality</h2>
<p><strong>Description</strong>: The following items describe tests that
need to be done for testing the quality of data. This area of tests
mainly concern whether the data supplied is in the expected format, data
containing null values or outliers to make sure that the data processing
pipeline is robust.</p>
<h3 id="validate-data-shape-and-values">3.1 Validate Data Shape and
Values</h3>
<p><strong>Requirement</strong>: Check that the data has the expected
shape and that all values meet domain-specific constraints, such as
non-negative distances.</p>
<p><strong>Explanation</strong>: Check that the data being used has the
correct structure (like having the right number of columns) and that the
values within the data make sense (e.g., distances should not be
negative). This ensures that the data is valid and reliable for model
training.</p>
<p><strong>References:</strong></p>
<ul>
<li>alexander2024Evaluating</li>
<li>ISO/IEC5259</li>
</ul>
<h3 id="check-for-duplicate-records-in-data">3.2 Check for Duplicate
Records in Data</h3>
<p><strong>Requirement</strong>: Check for duplicate records in the
dataset and ensure that there are none.</p>
<p><strong>Explanation</strong>: Ensure that the dataset does not
contain duplicate entries, as these can skew the results and reduce the
model's performance. The test should identify any repeated records so
they can be removed or investigated.</p>
<p><strong>References:</strong></p>
<ul>
<li>ISO/IEC5259</li>
</ul>
<h2 id="data-ingestion">4 Data Ingestion</h2>
<p><strong>Description</strong>: The following items describe tests that
need to be done for testing if the data is ingestion properly.</p>
<h3 id="verify-data-split-proportion">4.1 Verify Data Split
Proportion</h3>
<p><strong>Requirement</strong>: Check that the data is split into
training and testing sets in the expected proportion.</p>
<p><strong>Explanation</strong>: Confirm that the data is divided
correctly into training and testing sets according to the intended
ratio. This is crucial for ensuring that the model is trained and
evaluated properly, with representative samples in each set.</p>
<p><strong>References:</strong></p>
<ul>
<li>openja2023studying</li>
<li>DBLP:conf/recsys/Kula15</li>
<li>singh2020mmf</li>
</ul>
<h2 id="model-fitting">5 Model Fitting</h2>
<p><strong>Description</strong>: The following items describe tests that
need to be done for testing the model fitting process. The unit tests
written for this section usually mock model load and model predictions
similarly to mocking file access.</p>
<h3 id="test-model-output-shape">5.1 Test Model Output Shape</h3>
<p><strong>Requirement</strong>: Validate that the model's output has
the expected shape.</p>
<p><strong>Explanation</strong>: Ensure that the output from the model
has the correct dimensions and structure. For example, in a
classification task, if the model should output probabilities for each
class, the test should verify that the output is an array with the
correct dimensions. Ensuring the correct output shape helps prevent
runtime errors and ensures consistency in how data is handled
downstream.</p>
<p><strong>References:</strong></p>
<ul>
<li>openja2023studying</li>
<li>DBLP:conf/recsys/Kula15</li>
<li>singh2020mmf</li>
</ul>
<h2 id="model-evaluation">6 Model Evaluation</h2>
<p><strong>Description</strong>: The following items describe tests that
need to be done for testing the model evaluation process.</p>
<h3 id="verify-evaluation-metrics-implementation">6.1 Verify Evaluation
Metrics Implementation</h3>
<p><strong>Requirement</strong>: Verify that the evaluation metrics are
correctly implemented and appropriate for the model's task.</p>
<p><strong>Explanation</strong>: Confirm that the metrics used to
evaluate the model are implemented correctly and are suitable for the
specific task at hand. This helps in accurately assessing the model's
performance and understanding its strengths and weaknesses.</p>
<p><strong>References:</strong></p>
<ul>
<li>openja2023studying</li>
<li>DBLP:conf/recsys/Kula15</li>
<li>singh2020mmf</li>
</ul>
<h3 id="evaluate-models-performance-against-thresholds">6.2 Evaluate
Model’s Performance Against Thresholds</h3>
<p><strong>Requirement</strong>: Compute evaluation metrics for both the
training and testing datasets and ensure that these metrics exceed
predefined threshold values, indicating acceptable model
performance.</p>
<p><strong>Explanation</strong>: This ensures that the model's
performance meets or exceeds certain benchmarks. By setting thresholds
for metrics like accuracy or precision, you can automatically flag
models that underperform or overfit. This is crucial for maintaining a
baseline quality of results and for ensuring that the model meets the
requirements necessary for deployment.</p>
<p><strong>References:</strong></p>
<ul>
<li>openja2023studying</li>
<li>DBLP:conf/recsys/Kula15</li>
<li>singh2020mmf</li>
</ul>
<h2 id="artifact-testing">7 Artifact Testing</h2>
<p><strong>Description</strong>: The following items involves explicit
checks for behaviors that we expect the artifacts e.g. models, plots,
etc., to follow.</p>
<h2 id="data-quality-optional">8 Data Quality (Optional)</h2>
<p><strong>Description</strong>: The following items describe tests that
need to be done for testing the quality of data, but they may not be
applicable to all projects.</p>
<h3 id="validate-outliers-detection-and-handling">8.1 Validate Outliers
Detection and Handling</h3>
<p><strong>Requirement</strong>: Detect outliers in the dataset. Ensure
that the outlier detection mechanism is sensitive enough to flag true
outliers while ignoring minor anomalies.</p>
<p><strong>Explanation</strong>: The detection method should be precise
enough to catch significant anomalies without being misled by minor
variations. This is important for maintaining data quality and ensuring
the model's reliability in certain projects.</p>
<p><strong>References:</strong></p>
<ul>
<li>ISO/IEC5259</li>
</ul>
