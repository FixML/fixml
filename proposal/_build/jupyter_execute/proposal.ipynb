{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklists and LLM prompts for efficient and effective test creation in data analysis - Initial Proposal\n",
    "- Authored By John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin\n",
    "\n",
    "# 1.0 EXECUTIVE SUMMARY\n",
    "\n",
    "The rapid growth of global artificial intelligence (AI) market presents both opportunities and challenges. While AI systems have the potential to impact various aspects of human life, ensuring their software quality remains a significant concern. Current testing strategies for machine learning (ML) systems lack standardization and comprehensiveness, which poses risks to stakeholders such as financial losses and safety hazards.\n",
    "\n",
    "Our proposal addresses this challenge by developing an end-to-end application that provides comprehensive test evaluation, tailored test recommendations, and automated test specifications generation. Through these features, users can systematically assess, improve, and include tests tailored to their ML systems. By leveraging human expertise and prompt engineering, we build our product to deliver actionable insights for improving users' test strategies and overall ML system reliability.\n",
    "\n",
    "With an iterative development approach, we aim to develop our minimum viable product (MVP) in the first two weeks. We will further iterate and refine our product over the next 3 weeks. During the last 2 weeks, we will carry out rigorous system testing, finalize our final product and report, and deliver to our partners. Ultimately, our goal is to mitigate potential negative societal impacts associated with unreliable ML systems and promote trustworthiness.\n",
    "\n",
    "# 2.0 INTRODUCTION\n",
    "\n",
    "## 2.1 Problem Statement\n",
    "Global artificial intelligence (AI) market is growing exponentially {cite}`grand2021artificial`, which is driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis, etc. \n",
    "\n",
    "However, ensuring the software quality of these systems is yet an open challenge {cite}`openja2023studying`. A lack of a standardized and comprehensive approach to testing machine learning (ML) systems is observed, which might pose potential risks to stakeholders. Inadequate quality assurance in ML systems could possibly link to severe consequences such as financial losses {cite}`Asheeta2019` and safety hazards, which raises our concerns about the need for robust testing methodologies.\n",
    "\n",
    "## 2.2 Our Objectives\n",
    "Quality assurance in software engineering is dependent on testing suites. Our proposal aims to enhance the trustworthiness and robustness of applied ML software through the development of comprehensive ML system testing suites. We seek to improve the quality and reproducibility of ML systems {cite}`kapoor2022leakage` in both industry and academia via the systematic testing approach. Ultimately, our goal is to mitigate the potential negative societal impacts associated with unreliable ML systems.\n",
    "\n",
    "# 3.0 OUR PRODUCT\n",
    "Our solution offers an end-to-end application for evaluating and enhancing the robustness of users' machine learning (ML) systems. Key features include:\n",
    "1. Comprehensive Test Evaluation\n",
    "2. Tailored Test Recommendations\n",
    "3. Automated Test Specification Generation\n",
    "\n",
    "## 3.1 Description\n",
    "Basically, our product can be utilized into 3 stages.\n",
    "\n",
    "- Stage 1: Comprehensive Test Evaluation\n",
    "\n",
    "    Users input the source code of their ML systems along with our suggested prompts into the application, which utilizes Large Language Model (LLM) to identify existing test suites within the code. A comprehensive report is generated and provides qualitative (e.g., test categories/strategies covered) and quantitative (e.g., test coverage/score) evaluations of the ML system's test quality.\n",
    "\n",
    "- Stage 2: Tailored Test Recommendations\n",
    "\n",
    "    Based on the robust checklist created by our team for testing applied ML code, the application assesses the adequacy of existing tests. Recommendations on additional test categories/strategies are provided based on the ML system's nature and test evaluation for improvement.\n",
    "\n",
    "- Stage 3: Automated Test Specification Generation\n",
    "\n",
    "    Users specify desired test categories/strategies and utilize our suggested LLM prompts within the application. The application autonomously engineers reproducible test data and software tests for reference, which serve as reliable starting points for users to utilize and improve their test suites within the ML systems.\n",
    "\n",
    "## 3.2 Success Metrics\n",
    "\n",
    "The success of our product will be dependent on the mutation testing result of the reference test cases generated by the application. A set of perturbations would be made to the ML project code and the success rate of detecting these perturbations by the implemented test cases will be recorded as the evaluation metric.\n",
    "\n",
    "Our partner and stakeholders would expect to see a significant improvement in testing strategies of their ML systems post-application usage. Moreover, the application would demonstrate high accuracy in detecting faults, which ensure consistent and high quality ML projects upon updates. \n",
    "\n",
    "## 3.3 Data Science Approach\n",
    "\n",
    "### 3.3.1 Data \n",
    "\n",
    "We will collect data from the 377 GitHub repositories identified in the study by {cite}`wattanakriengkrai2022github`. The data will include repository metadata and source code sourced using GitHub API and custom scripts. To ensure relevance to our study, we will apply these criteria for filtering the data: 1) projects that are related to ML systems; 2) projects that contain test cases; 3) test cases are written in Python programming language.\n",
    "\n",
    "{screencap/copy-and-paste of the code}\n",
    "\n",
    "### 3.3.2 Methodologies\n",
    "Our data science methodology incorporates both human expert evaluation and prompt engineering to assess and enhance the test quality of ML systems.\n",
    "\n",
    "- Human Expert Evaluation\n",
    "\n",
    "    We will begin by formulating a comprehensive checklist for evaluating the data and ML pipeline based on established testing strategies outlined in {cite}`openja2023studying` as the foundational framework. for assessing test quality within selected repositories. Our team will manually evaluate the test quality within the repository data based on the formulated checklist. The checklist will be refined during the process to ensure its applicability and robustness testing general ML systems.\n",
    "\n",
    "- Prompt Engineering\n",
    "\n",
    "    We will engineer prompts for LLM to serve various purposes across three stages:\n",
    "    1. Prompts to examine test cases within ML system source codes and deliver qualitative and quantitative test scores.\n",
    "    2. Prompts incorporated with the completed checklist to suggest potential testing strategies by comparing with ML system source codes.\n",
    "    3. Prompts to generate test cases based on suggested testing strategies and ML system task types {cite}`schafer2023empirical`\n",
    "\n",
    "### 3.3.3 Iterative Development Approach\n",
    "As we leverage data from selected GitHub repositories and references research on testing strategies, it's important to acknowledge that this may not include all ML systems or testing methodologies. To address these considerations, we adopt an iterative development approach by setting up an open and scalable framework for this project. Our application could then undergo continuous updates based on users' feedback and contributors' insights.\n",
    "\n",
    "We encourage users to interpret the artifacts generated by the application with a grain of salt and recognize the evolving nature of ML system testing practices.\n",
    "\n",
    "# 4.0 DELIVERY TIMELINE\n",
    "Our team follow a timeline for our product delivery. We also aim at close communication with our partner to align our product development with their expectation.\n",
    "\n",
    "| Timeline | Milestones |\n",
    "|---|---|\n",
    "| Week 1 (Apr 29 - May 3) | Prepare and Present Initial Proposal. Scrape repository data. |\n",
    "| Week 2 - 3 (May 6 - 17) | Deliver Proposal. Deliver Draft of ML Pipeline Test Checklist. Develop Minimum Viable Product (Test Completeness Score, Missing Test Recommendation) |\n",
    "| Week 4 - 5 (May 20 - May 31) | Update Test Checklist. Develop Test Function Spec Generator. |\n",
    "| Week 6 (Jun 3 - Jun 7) | Update Test Checklist. Wrap Up Product. |\n",
    "| Week 7 (Jun 10 - Jun 14) | Finalize Test Checklist. Perform Product System Test. Present Final Product. Prepare Final Product Report. |\n",
    "| Week 8 (Jun 17 - Jun 21) | Deliver Final Product. Deliver Final Product Report. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}