---
title: "DSCI591 Capstone Final Report"
format:
  html:
    code-fold: true
bibliography: references.bib
---

# Final Report - Checklists and LLM prompts for efficient and effective test creation in data analysis

by John Shiu, Orix Au Yeung, Tony Shum, Yingzi Jin

## Executive Summary

#FIXME

## Introduction

### Problem Statement

The global artificial intelligence (AI) market is growing exponentially ([@grand2021artificial]), driven by its ability to autonomously make complex decisions impacting various aspects of human life, including financial transactions, autonomous transportation, and medical diagnosis.

However, ensuring the software quality of these systems remains a significant challenge ([@openja2023studying]). Specifically, the lack of a standardized and comprehensive approach to testing machine learning (ML) systems introduces potential risks to stakeholders. For example, inadequate quality assurance in ML systems can lead to severe consequences, such as misinformation ([@Ashley2024]), social bias ([@Alice2023]), substantial financial losses ([@Asheeta2019]) and safety hazards.

Therefore, defining and promoting an industry standard and establishing robust testing methodologies for these systems is crucial. But how?

### Our Objectives

We propose to develop testing suites diagnostic tools based on Large Language Models (LLMs) and curate checklists based on ML research papers and best practices to facilitate comprehensive testing of ML systems with flexibility. Our goal is to enhance applied ML software's trustworthiness, quality, and reproducibility across both the industry and academia [@kapoor2022leakage].

## Data Science Methods

### Current Approaches

To ensure the reproducibility, trustworthiness and free-of-bias ML system, comprehensive testing is essential. We have observed some traditional approaches in assessing the completeness of ML system tests, which contain different advantages and drawbacks as follows.

1. **Code Coverage**

Code coverage is a measure of the proportion of source code of a program executed when a particular test suite is run. It is widely used in software development domain as one of the measurements. It quantifies the test quality and is scalable given the short processing time. However, it cannot provide the reasons and in which ML areas that the test suites fall short under the context of ML system development.

2. **Manual Evaluation**

Manual evaluation involves human expert review at the source code, whom can take the business logic into considerations and find vulnerabilites. Manual evaluation usually delivers comments for improvement under specific development context, and it is still one of the most reliable methods in practice ([@openja2023studying], [@alexander2023evaluating]). However, the time cost is large and it is not scalable due to the scarcity of time and human expert. Different human expert might put emphasis on different ML test areas instead of a comprehensive and holistic review on the ML system test suites.

### Our Approach

Our approach is to deliver an automated code review tool with the best practices of ML test suites embedded, which can be used by ML users to learn the best practices as well as to obtain a comprehensive evaluation on their ML system codes.

To come up with the best practices of ML test suites, ML research paper and recognized online resources are our data. Under the collaboration with our partner, we have researched industrial best practices ([@msise2023], [@jordan2020]) and published academic literature ([@openja2023studying]) and consolidated the testing strategies of ML projects into a checklist which is easily legible and editable by human (researchers, ML engineers, etc.). The checklist is also machine-friendly that can be embedded into the automated tool.

To develop our automated code review tool, GitHub repositories of ML projects are our data. We have collected 11 repositories studied in [@openja2023studying], where these projects include comprehensive test suites and are written in Python programming language, for our product development. Our tool is capable of understanding the test suites in these projects, comparing and contrasting the test suites with the embedded best practices, and delivering evaluations to the current test suites.

By developing our approach, we expect that it can provide reliable test suites evaluation to multiple ML projects in a scalable manner. However, we acknowledged that the consolidation of best practices currently focused on a few high priority test areas due to time constraint, where we expect to expand in the future. The test evaluation results provided by our tool are yet as reliable as human evaluation, where we will quantify its performance using the success metrics below.

### Success Metrics

To properly assess the performance of our tool which leverages the capability of LLMs, we have researched and taken reference of the methods in [@alexander2023evaluating] and defined the 2 success metrics: accuracy and consistency. With these metrics, our users (researchers, ML engineers, etc.) can assess the degree of trustworthiness of the evaluation results from our tool.

1.  **Accuracy of the Application vs Human Expert Judgement**

We run our tool on the ML projects in [@openja2023studying] to obtain the evaluation results (i.e. completeness score) per each ML test best practice item. We then manually assess the test suites of these ML projects using the same criteria as the ground truth data. Machine evaluation results are compared and contrasted with the ground truth data. Accuracy is defined as the number of matching results over total number of results.

2.  **Consistency of the Application**

Multiple runs on each ML project are performed and the evaluation results per each checklist item are obtained. Standard deviation of these results per ML projects are calculated as a measure of consistency.

## Data Product & Results

### Data Products

Our solution offers both a curated checklist on robust ML testing, and a Python package that facilitates the use of LLMs in checklist-based evaluation on the robustness of users' ML projects. The Python package is made publicly available for distribution on the Python Packaging Index (PyPI).

The justifications for creating these products are, on one hand, checklists have been shown to decrease errors in software systems and promote code submissions ([@Atul2010],  [@pineau2021improving]). Moreover, Python is chosen to be the programming language of our package given its prevalence in the ML landscape, its ubiquitous presence across different OSes and the existence of Python libraries for the integration with LLMs. This lowers the barrier to use and develop our package and provides better user experience.

#### How to use the product

There are two ways to make use of this package:

1.  **As a CLI tool.** A runnable command `fixml` is provided by the package. Once installed, users can perform the codebase evaluation, test function specification generation and other relevant tasks by running subcommands under `fixml` in terminal environment.

2.  **As a high-level API.** Alternatively, one can use the package to import all components necessary for performing the tasks as part of their own system. Documentations are provided in terms of docstrings.

By formatting our product as a CLI tool and API, one (researchers, ML engineers, etc.) will find it user-friendly to interact with, and versatile to support various use cases, e.g. web application development, scientific research, etc.

#### System Design

(FIXME To be revised) ![image](../../img/proposed_system_overview.png)

The design principle of our package adheres to object-oriented design and SOLID principles, which is fully modular. One can easily switch between different prompts, models and checklists to use. This facilitates code reusability and users' collaboration to extend its functionality.
The design principle of our package adheres to object-oriented design and SOLID principles, which is fully modular. One can easily switch between different prompts, models and checklists to use. This facilitates code reusability and users' collaboration to extend its functionality.

There are five components in the system of our package:

1.  **Code Analyzer** 
This component extracts the information relevant to test suites from the input codebase, which is essential for injecting only the most relevant information to LLMs given its token limits.

2.  **Prompt Templates** 
This component stores the prompt template necessary for instructing LLMs to behave and return responses in the expected format.

3.  **Checklist** 
This component reads the curated checklist, which is stored in CSV format, as a dictionary with fixed schema for injection to LLMs. Default checklist is also included inside the package for distribution.

4.  **Runners** 
This component involves the Evaluator module, which evaluates each file from the test suites using LLMs and outputs evaluation results, and Generator module, which generates test specifications. Both modules include validation and retry logics and record all relevant information in the responses.

5.  **Parsers** 
This components parses the responses from Evaluator into evaluation reports in various formats (HTML, PDF) using Jinja template engine. Adhering to our design principle, this enables flexibility in creating customized report structure.

#### Checklist Design

The checklist ([Fig. 1](overview-diagram)) embedded in the package contains the best practices in testing ML pipeline and is curated manually based on ML researches and recognized online resources. Prompt engineering is applied for better performance. This also helps combating the hallucination of LLMs ([@zhang2023sirens]) during the evaluation of ML projects by prompting it to follow **exactly** the checklist.

Here is an example of how the checklist would be structured:

|                  Column | Description                                                                                          |
|------------------:|:----------------------------------------------------|
|                      ID | The Unique Identifier of the checklist item                                                          |
|                   Topic | The Test Area of the checklist item                                                                  |
|                   Title | The Title of the checklist item                                                                      |
|             Requirement | The Prompt of the checklist item to be injected into LLMs for evaluation                             |
|            Explanations | Detailed explanations of the checklist item for human understanding                                  |
|               Reference | References of the checklist item, e.g. academic paper                                                |
| Is Evaluator Applicable | Whether the checklist item is selected to be used during evaluation. 0 indicates No, 1 indicates Yes |

(FIXME To be revised) <img src="../../img/checklist_sample.png" width="200" />

#### Artifacts

There are three artifacts after using our package:

1.  **Evaluation Responses** 
The artifact stores both the evaluation responses from LLMs and meta-data of the process in JSON format. This supports downstream tasks, e.g. report render, scientific research, etc.

(FIXME To be revised) schema of the JSON saved & what kind of information is stored

2.  **Evaluation Report** 
The artifact stores the evaluation results of the ML projects in a structured format, which includes completeness score breakdown and corresponding detailed reasons.

(FIXME To be revised) <img src="../../img/test_evaluation_report_sample.png" width="200" />

3.  **Test Specification Script** 
The artifacts stores the test specifications generated by LLMs in Python script format.

(FIXME To be revised) <img src="../../img/test_spec_sample.png" width="200" />

### Evaluation Results

As illustrated in `Success Metrics`, we ran 30 iterations on each of the repositories in [@openja2023studying] and examined the breakdown of the completeness score to assessed the quality of evaluation determined by our tool. 

(FIXME: would it be better to show a table of the repos? like how the Openja does?) 

1. **Accuracy**

For accuracy, we targeted 3 of the repositories ([`lightfm`](https://github.com/lyst/lightfm), [`qlib`](https://github.com/microsoft/qlib), [`DeepSpeech`](https://github.com/mozilla/DeepSpeech)) for human evaluation and compared the ground truth with the outputs from our tool.

```{python}
import pandas as pd
gt = pd.read_csv('ground_truth.csv')
gt
```
> Caption: Ground truth data on the 3 repositories. 1 = fully satisfied, 0.5 = partially satisfied, 0 = not satisfied

```{python}
# FIXME: jitter-mean-sd plot (checklist item vs. score) for each repo
import altair as alt
import pandas as pd

df_repo__stat = pd.read_csv('score_stat_by_repo_3.5-turbo.csv')
gt = pd.read_csv('ground_truth.csv')
gt = gt.melt(id_vars=['id', 'title'], var_name='repo', value_name='ground_truth')

df_repo__stat_with_gt = df_repo__stat.merge(gt, on=['id', 'title', 'repo'])

base = alt.Chart(
    df_repo__stat_with_gt.query('repo in ["lightfm", "qlib", "DeepSpeech"]')
).transform_calculate(
    min="max(0, datum.mean-datum.std)",
    max="min(1, datum.mean+datum.std)"
)
    
# generate the points
points = base.mark_point(
    filled=True,
    size=50,
    color='black'
).encode(
    x=alt.X('mean:Q').scale(domainMin=0, domainMax=1).title("Score").axis(
        labelExpr="datum.value % 0.5 ? null : datum.label"
    ),
    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))#.scale(domainMin=0, domainMax=1).title('Score'),
)

# generate the points for ground truth
gt_points = base.mark_point(
    filled=True,
    size=200,
    color='green',
    shape="diamond"
).encode(
    x=alt.X('ground_truth:Q'),
    y=alt.Y('id_title:N')
)

# generate the error bars
errorbars = base.mark_errorbar().encode(
    x=alt.X("min:Q").title('1 SD'), #"id:N",
    x2="max:Q",
    y="id_title:N"
)

(gt_points + points + errorbars).facet(
    column=alt.Column('repo:N').title(None)
).configure_axis( 
    labelFontSize=12, 
    titleFontSize=12
)
```
> Caption: Comparison of the satisfaction determined by our system versus the ground truth for each checklist item and repository

We found that our tool tends to undermine the actual satisfying cases. For the items that are actually satisfied, our tool tends to classify as partially satisfied, while for those that are partially satisfied, our tool often classifies as not satisfied.

```{python}
df_repo_run = pd.read_csv('score_by_repo_run_3.5-turbo.csv')

df_repo_run = df_repo_run.merge(gt, on=['id', 'title', 'repo'])

contingency_table = pd.pivot_table(
    df_repo_run,
    values='run', 
    index=['repo', 'id_title', 'ground_truth'], 
    columns=['score'],
    aggfunc='count', 
    fill_value=0
)
contingency_table.index.names = ['Repository', 'Checklist Item', 'Ground Truth']
contingency_table.sort_index(level=[0, 2])
```
> Contingency table of the satisfaction determined by our system versus the ground truth

The accuracy issue may be attributed to the need for improvement of prompts in our checklist.

2. **Consistency**

Since the completeness score from LLMs contain randomness, we further studied the consistency of scores across checklist items and repositories.
Since the completeness score from LLMs contain randomness, we further studied the consistency of scores across checklist items and repositories.

```{python}
stds = df_repo__stat[['repo', 'std', 'id_title']].pivot(index='repo', columns='id_title').copy()
stds.columns = [col[1] for col in stds.columns]
stds = stds.reset_index()
stds = stds.melt(id_vars='repo', var_name='id_title')

base = alt.Chart(stds)

box = base.mark_boxplot(
    color='grey',
    opacity=0.5,
    size=20,
).encode(
    x=alt.X('value:Q').title('Standard Deviation of Scores'),
    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))
)

stripplot = base.mark_circle(size=100).encode(
    y=alt.Y( 
        'id_title:N',
        axis=alt.Axis(ticks=False, grid=True, labels=True), 
        scale=alt.Scale(), 
    ), 
    x='value:Q',
    yOffset="jitter:Q",
    color=alt.Color('id_title:N', legend=None),
    tooltip='repo'
).transform_calculate(
    # Generate Gaussian jitter with a Box-Muller transform
    jitter="sqrt(-2*log(random()))*cos(2*PI*random())"
)

(
    box + stripplot
).configure_view( 
    stroke=None
).configure_axis( 
    labelFontSize=12, 
    titleFontSize=12
).properties(
    height=300, 
    width=600,
    title="30 Runs on Openja's Repositories for each Checklist Item"
) 
```
> Caption: Standard deviations of the score for each checklist item. Each dot represents the standard deviation of scores of 30 runs of a single repository
> Caption: Standard deviations of the score for each checklist item. Each dot represents the standard deviation of scores of 30 runs of a single repository

We found 2 diverging cases. For example, it shows high standard deviations across repositories for item `3.2 Data in the Expected Format`. This might be a proof of poor prompt quality, making it ambiguous for the LLM to produce consistent results. Prompt engineering might solve this problem.

On the other hand, there are outliers yielding exceptionally high standard deviations for item `5.3 Ensure Model Output Shape Aligns with Expectation`. This might be because those repositories are unorthodox, but careful manual examination is required for a more definite conclusion.

#### Comparison of `gpt-3.5-turbo` and `gpt-4o`

To examine if newer LLMs help in both metrics, we preliminarily compared system outputs from `gpt-4o` versus `gpt-3.5-turbo` on the `lightfm` repository. We observed that the one from `gpt-4o` consistently returned "Satisfied", which deviates from the ground truth.

```{python}
# FIXME: jitter-mean-sd plot (checklist item vs. score) for each repo
df_repo_4o__stat = pd.read_csv('score_stat_by_repo_4o.csv')
df_repo_4o__stat_with_gt = df_repo_4o__stat.merge(gt, on=['id', 'title', 'repo'])
df_repo_4o__stat_with_gt['model'] = 'gpt-4o'

df_repo_35turbo__stat_with_gt = df_repo__stat_with_gt.query("repo == 'lightfm'").copy()
df_repo_35turbo__stat_with_gt['model'] = 'gpt-3.5-turbo'

df_model_comp = pd.concat(
    (df_repo_35turbo__stat_with_gt, df_repo_4o__stat_with_gt), 
    axis=0
)

base = alt.Chart(
    df_model_comp
).transform_calculate(
    min="max(0, datum.mean-datum.std)",
    max="min(1, datum.mean+datum.std)"
)
    
# generate the points
points = base.mark_point(
    filled=True,
    size=50,
    color='black'
).encode(
    x=alt.X('mean:Q').scale(domainMin=0, domainMax=1).title("Score").axis(
        labelExpr="datum.value % 0.5 ? null : datum.label"
    ),
    y=alt.Y('id_title:N', title=None, axis=alt.Axis(labelPadding=10, labelLimit=1000, grid=False))#.scale(domainMin=0, domainMax=1).title('Score'),
)

# generate the points for ground truth
gt_points = base.mark_point(
    filled=True,
    size=200,
    color='green',
    shape="diamond"
).encode(
    x=alt.X('ground_truth:Q'),
    y=alt.Y('id_title:N')
)

# generate the error bars
errorbars = base.mark_errorbar().encode(
    x=alt.X("min:Q").title('1 SD'), #"id:N",
    x2="max:Q",
    y="id_title:N"
)

(gt_points + points + errorbars).facet(
    column=alt.Column('model:N').title(None)
).configure_axis( 
    labelFontSize=12, 
    titleFontSize=12
)
```
> Caption: Comparison of the satisfaction using `gpt-4o` versus using `gpt-3.5-turbo` for each checklist item on `lightfm`

Further investigation into `gpt-4o` is required to address if it can enhance the system performance.

## Conclusion

### Wrap Up

The development of FixML have been driven by both the need of better quality assurance in ML systems, and the current limitations of traditional testing methods on ML projects. FixML provides curated checklists and automated tools that enhance the evaluation and creation of test suites for ML projects, which in return, significantly reduces the time and human effort required to assess the completeness of ML test suites. This popularizes thorough and efficient assessment on ML projects.

### Limitation & Future Improvement

While FixML provides substantial benefits, there are limitations and areas to be addressed in future development:

1.  **Specialized Checklist**

The default checklist is general and might not cover all requirements for different ML projects. Future development will focus on creating more specialized checklists for more tailored evaluations across domains and project types. We welcome any collaboration with ML researchers on the creation of specalized checklists based on their use cases.

2.  **Enhanced Test Evaluator**

Our study reveals the accuracy and consistency issues on the evaluation results using OpenAI GPT-3.5-turbo model. Future improvements involves better prompt engineering techniques and support for multiple LLMs for enhanced performance and flexibility. We expect to include user guidelines in prompt creation to faciliate collaboration with ML developers.

3.  **Customized Test Specification**

FixML currently produces general test function skeletons solely based on the curated checklist. Future developments involve the integration of the ML project infromation in the generation process to produce customized test functions skeletons. This further incentivizes users to create comprehensive tests.

4.  Workflow Optimization #FIXME: have to review whether to include as it seems lower priority.

The current test evaluator and test specification generator are separate entities. This could be improved by embedding a workflow engine that allows the system to automatically take actions based on the LLM response. For instance, if the LLM response suggests that test suites are partially satisfied or non-satisfied, the system could automatically run the test generator to produce test function skeletons and then reevaluate them until they are satisfied or some threshold is met. This would create a more cohesive and efficient workflow, reducing manual intervention and improving overall system performance.

5.  Performance Optimization #FIXME: have to review whether to include as it seems lower priority.

Performance optimization is another critical area for future development. As FixML handles large codebases and complex evaluations, optimizing the system to handle these tasks more efficiently is essential. This includes improving the speed and accuracy of the LLM responses, reducing the time taken to analyze and generate reports, and ensuring the system can scale effectively to handle more extensive and more complex projects.

By addressing these limitations with future improvements, we hope FixML achieves better performance and promotes better ML systems and thus better human life.

## References