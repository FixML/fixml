{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f427397a-321d-4ba8-ba63-512e18eea528",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb67c7b-0b04-4f5f-a42b-7b31dcd963bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b6521a-4c59-4c34-ae5f-720706d2f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import TestEvaluator\n",
    "from modules.checklist.checklist import *\n",
    "\n",
    "import pandas as pd\n",
    "import pypandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a4642-ffbb-49a8-ab2c-8503d6bc58aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c07c26fc-acaf-4e3d-a470-60077611c526",
   "metadata": {},
   "source": [
    "## Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18bfc5a-83d0-4078-89db-d07894d8357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checklist_path = '../../checklist/checklist_demo.csv'\n",
    "# checklist = Checklist(checklist_path, checklist_format=ChecklistFormat.CSV)\n",
    "# checklist.as_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67820a8-b826-45ec-b596-b8b6ea59e137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'a':[1, 2], 'b':[3, 4]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2139ca-98d3-4253-a2e1-3ecdce1a1018",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a03e63c-fb18-4361-b117-aa2355b7f5bb",
   "metadata": {},
   "source": [
    "Please specify the `test_functions_directory` below to load the ML test code base, the parameters, e.g. checklist, and the corresponding models to for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4408d48-9590-444c-8725-52b06363fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74036e8d-1dee-459b-bdf7-fa797b262e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_functions_directory = '../../../lightfm/tests'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210373f1-9354-434c-9103-5c4b767b14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperatures = [0.1]\n",
    "# models = ['gpt-3.5-turbo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457946ba-0723-45a1-9491-c40ede92992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist_directory = '../../checklist/checklist_demo.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc01e5b0-f3d7-4230-b413-e94372d88634",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'checklist_demo_1'\n",
    "evaluator = TestEvaluator(test_functions_directory)\n",
    "evaluator.load_checklist(checklist_directory)\n",
    "models.append({'name': name, 'model': evaluator})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5c0a6e-5047-4599-b898-9938f6297115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:13<00:00,  6.95s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2bf9b0b-8f6e-44de-8370-f4588228aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dict()\n",
    "result['score'] = evaluator.get_completeness_score(score_format='number')\n",
    "result['report'] = evaluator.evaluation_report\n",
    "result['result'] = evaluator.evaluation_result\n",
    "result['temp'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "291e5b34-6038-474b-bb1a-6ce0fcb1982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBC_FIXME From checklist.py. To be refactored \n",
    "def _get_md_representation(content: dict, curr_level: int):\n",
    "    repeated_col = [k for k, v in content.items() if isinstance(v, list)]\n",
    "\n",
    "    # print out header for each item\n",
    "    md_repr = '#' * curr_level\n",
    "    if 'ID' in content.keys():\n",
    "        md_repr += f\" {content['ID']}\"\n",
    "    if 'Title' in content.keys():\n",
    "        md_repr += f\" {content['Title']}\\n\\n\"\n",
    "    elif 'Topic' in content.keys():\n",
    "        md_repr += f\" {content['Topic']}\\n\\n\"\n",
    "\n",
    "    # print out non-title, non-repeated items\n",
    "    for k, v in content.items():\n",
    "        if k not in repeated_col and k not in ['Title', 'Topic', 'ID']:\n",
    "            md_repr += f'**{k}**: {v}\\n\\n'\n",
    "\n",
    "    # handle repeated columns and references\n",
    "    point_form_cols = ['References', 'Function References', 'Observations']\n",
    "    for k in repeated_col:\n",
    "        if k not in point_form_cols:\n",
    "            for item in content[k]:\n",
    "                md_repr += _get_md_representation(item, curr_level=curr_level + 1)\n",
    "        else:\n",
    "            md_repr += f'**{k}:**\\n\\n' + '\\n'.join(f'  - {item}' for item in content[k]) + '\\n\\n'\n",
    "\n",
    "    return md_repr\n",
    "\n",
    "# TBC_FIXME From checklist.py. To be refactored \n",
    "@staticmethod\n",
    "def __filedump_check(output_path: str, exist_ok: bool):\n",
    "    if not exist_ok and os.path.exists(output_path):\n",
    "        raise FileExistsError(\"Output file already exists. Use `exist_ok=True` to overwrite.\")\n",
    "    return True\n",
    "\n",
    "# TBC_FIXME From checklist.py. To be refactored \n",
    "def export_html(content: str, output_path: str, exist_ok: bool = False):\n",
    "    # self.__filedump_check(output_path, exist_ok)\n",
    "    pypandoc.convert_text(content, 'html', format='md', outputfile=output_path, extra_args=['--from=gfm'])\n",
    "\n",
    "# TBC_FIXME From checklist.py. To be refactored \n",
    "def export_pdf(content: str, output_path: str, exist_ok: bool = False):\n",
    "    # self.__filedump_check(output_path, exist_ok)\n",
    "    pypandoc.convert_text(content, 'pdf', format='md', outputfile=output_path,\n",
    "                          extra_args=['--pdf-engine=tectonic'])\n",
    "\n",
    "def export_evaluation_report(output_path, format='html', exist_ok: bool = False):\n",
    "    # if self.evaluation_report is None:\n",
    "    #     raise NotImplementedError(\n",
    "    #         # TBC_FIXME\n",
    "    #         \"Evaluation report is not yet created. Please make sure the function `get_completeness_score` is run before calling this export function\"\n",
    "    #     )\n",
    "\n",
    "    score = result['score']\n",
    "    summary_df = result['report'][['ID', 'Title', 'is_Satisfied', 'n_files_tested']]\n",
    "    details = result['report'][['ID', 'Title', 'Requirement', 'Observations', 'Function References']].to_dict(orient='records')\n",
    "\n",
    "    export_content = dict()\n",
    "    export_content['Title'] = 'Test Evaluation Report'\n",
    "    export_content['Report Areas'] = []\n",
    "    export_content['Report Areas'].append({'Title': 'Summary', 'Completeness Score': score, 'Completeness Score per Checklist Item': '\\n\\n' + summary_df.to_markdown(index=False)})\n",
    "    export_content['Report Areas'].append({'Title': 'Details', 'Report Detail': details})\n",
    "    if format=='html':\n",
    "        export_html(_get_md_representation(export_content, curr_level=1), output_path, exist_ok)\n",
    "    elif format=='pdf':\n",
    "        export_pdf(_get_md_representation(export_content, curr_level=1), output_path, exist_ok)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89ae7468-e43e-4937-a016-3e9f1740f41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Requirement</th>\n",
       "      <th>is_Satisfied</th>\n",
       "      <th>n_files_tested</th>\n",
       "      <th>Observations</th>\n",
       "      <th>Function References</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <th>Write Descriptive Test Names</th>\n",
       "      <td>Each test function should have a clear, descri...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[(test_fast_functions.py) The test function 't...</td>\n",
       "      <td>[{'File Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <th>Keep Tests Focused</th>\n",
       "      <td>Each test should focus on a single scenario, u...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[(test_fast_functions.py) The test function 't...</td>\n",
       "      <td>[{'File Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <th>Ensure Data File Loads as Expected</th>\n",
       "      <td>Ensure that data-loading functions correctly l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[(test_fast_functions.py) Not Applicable, nan,...</td>\n",
       "      <td>[{'File Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <th>Validate Model Input and Output Compatibility</th>\n",
       "      <td>Confirm that the model accepts inputs of the c...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[(test_fast_functions.py) The test functions v...</td>\n",
       "      <td>[{'File Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         Requirement  \\\n",
       "ID  Title                                                                                              \n",
       "1.1 Write Descriptive Test Names                   Each test function should have a clear, descri...   \n",
       "1.2 Keep Tests Focused                             Each test should focus on a single scenario, u...   \n",
       "2.1 Ensure Data File Loads as Expected             Ensure that data-loading functions correctly l...   \n",
       "5.1 Validate Model Input and Output Compatibility  Confirm that the model accepts inputs of the c...   \n",
       "\n",
       "                                                   is_Satisfied  \\\n",
       "ID  Title                                                         \n",
       "1.1 Write Descriptive Test Names                            1.0   \n",
       "1.2 Keep Tests Focused                                      1.0   \n",
       "2.1 Ensure Data File Loads as Expected                      1.0   \n",
       "5.1 Validate Model Input and Output Compatibility           1.0   \n",
       "\n",
       "                                                   n_files_tested  \\\n",
       "ID  Title                                                           \n",
       "1.1 Write Descriptive Test Names                                6   \n",
       "1.2 Keep Tests Focused                                          6   \n",
       "2.1 Ensure Data File Loads as Expected                          6   \n",
       "5.1 Validate Model Input and Output Compatibility               6   \n",
       "\n",
       "                                                                                        Observations  \\\n",
       "ID  Title                                                                                              \n",
       "1.1 Write Descriptive Test Names                   [(test_fast_functions.py) The test function 't...   \n",
       "1.2 Keep Tests Focused                             [(test_fast_functions.py) The test function 't...   \n",
       "2.1 Ensure Data File Loads as Expected             [(test_fast_functions.py) Not Applicable, nan,...   \n",
       "5.1 Validate Model Input and Output Compatibility  [(test_fast_functions.py) The test functions v...   \n",
       "\n",
       "                                                                                 Function References  \n",
       "ID  Title                                                                                             \n",
       "1.1 Write Descriptive Test Names                   [{'File Path': '../../../lightfm/tests/test_fa...  \n",
       "1.2 Keep Tests Focused                             [{'File Path': '../../../lightfm/tests/test_fa...  \n",
       "2.1 Ensure Data File Loads as Expected             [{'File Path': '../../../lightfm/tests/test_fa...  \n",
       "5.1 Validate Model Input and Output Compatibility  [{'File Path': '../../../lightfm/tests/test_fa...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a4c43f-0096-442b-97b8-7af20561ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../report/evaluation_report.html'\n",
    "evaluator.get_completeness_score()\n",
    "evaluator.export_evaluation_report(output_path, 'html', exist_ok=True)\n",
    "# export_evaluation_report(output_path, 'pdf', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93175369-6189-445c-9eda-7a5ed518e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result['report'].reset_index().to_dict(orient='records')\n",
    "summary_df = result['report'].reset_index()[['ID', 'Title', 'is_Satisfied', 'n_files_tested']]\n",
    "details = result['report'].reset_index()[['ID', 'Title', 'Requirement', 'References']].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64b14a2c-2ba6-4eab-bc74-e663d0ca62df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>is_Satisfied</th>\n",
       "      <th>n_files_tested</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>Write Descriptive Test Names</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2</td>\n",
       "      <td>Keep Tests Focused</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Validate Model Input and Output Compatibility</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                          Title  is_Satisfied  \\\n",
       "0  1.1                   Write Descriptive Test Names           1.0   \n",
       "1  1.2                             Keep Tests Focused           1.0   \n",
       "2  2.1             Ensure Data File Loads as Expected           0.5   \n",
       "3  5.1  Validate Model Input and Output Compatibility           1.0   \n",
       "\n",
       "   n_files_tested  \n",
       "0               2  \n",
       "1               2  \n",
       "2               2  \n",
       "3               2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8910f3d3-015d-4b8e-88e4-683d6d8b0d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'Test Evaluation Report',\n",
       " 'Report Areas': [{'Title': 'Summary',\n",
       "   'Completeness Score': 0.875,\n",
       "   'Completeness Score per Checklist Item': '\\n|   ID | Title                                         |   is_Satisfied |   n_files_tested |\\n|-----:|:----------------------------------------------|---------------:|-----------------:|\\n|  1.1 | Write Descriptive Test Names                  |            1   |                2 |\\n|  1.2 | Keep Tests Focused                            |            1   |                2 |\\n|  2.1 | Ensure Data File Loads as Expected            |            0.5 |                2 |\\n|  5.1 | Validate Model Input and Output Compatibility |            1   |                2 |'},\n",
       "  {'Title': 'Details',\n",
       "   'Report Detail': [{'ID': '1.1',\n",
       "     'Title': 'Write Descriptive Test Names',\n",
       "     'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "     'References': [{'File_Path': '../../../lightfm/tests/test_cross_validation.py',\n",
       "       'Functions': ['test_random_train_test_split']},\n",
       "      {'File_Path': '../../../lightfm/tests/test_data.py',\n",
       "       'Functions': ['test_fitting',\n",
       "        'test_fitting_no_identity',\n",
       "        'test_exceptions',\n",
       "        'test_build_features']}]},\n",
       "    {'ID': '1.2',\n",
       "     'Title': 'Keep Tests Focused',\n",
       "     'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "     'References': [{'File_Path': '../../../lightfm/tests/test_cross_validation.py',\n",
       "       'Functions': ['test_random_train_test_split']},\n",
       "      {'File_Path': '../../../lightfm/tests/test_data.py',\n",
       "       'Functions': ['test_fitting',\n",
       "        'test_fitting_no_identity',\n",
       "        'test_exceptions',\n",
       "        'test_build_features']}]},\n",
       "    {'ID': '2.1',\n",
       "     'Title': 'Ensure Data File Loads as Expected',\n",
       "     'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "     'References': [{'File_Path': '../../../lightfm/tests/test_cross_validation.py',\n",
       "       'Functions': ['test_random_train_test_split']},\n",
       "      {'File_Path': '../../../lightfm/tests/test_data.py', 'Functions': []}]},\n",
       "    {'ID': '5.1',\n",
       "     'Title': 'Validate Model Input and Output Compatibility',\n",
       "     'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "     'References': [{'File_Path': '../../../lightfm/tests/test_cross_validation.py',\n",
       "       'Functions': ['test_random_train_test_split']},\n",
       "      {'File_Path': '../../../lightfm/tests/test_data.py',\n",
       "       'Functions': ['test_fitting',\n",
       "        'test_fitting_no_identity',\n",
       "        'test_build_features']}]}]}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_content = dict()\n",
    "export_content['Title'] = 'Test Evaluation Report'\n",
    "export_content['Report Areas'] = []\n",
    "export_content['Report Areas'].append({'Title': 'Summary', \n",
    "                                            'Completeness Score': result['score'], \n",
    "                                            'Completeness Score per Checklist Item': '\\n' + summary_df.to_markdown(index=False)\n",
    "                                            })\n",
    "export_content['Report Areas'].append({'Title': 'Details', 'Report Detail': details})\n",
    "export_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3d964a0-6a9b-4f7a-88f5-89110bf44bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': '../../../lightfm/tests/test_fast_functions.py',\n",
       "  'report': [{'ID': '1.1',\n",
       "    'Title': 'Write Descriptive Test Names',\n",
       "    'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "    'Observation': \"The test function 'test_in_positives' has a descriptive name that indicates it tests for positive cases related to the input data.\",\n",
       "    'Functions': ['test_in_positives'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_fast_functions.py'},\n",
       "   {'ID': '1.2',\n",
       "    'Title': 'Keep Tests Focused',\n",
       "    'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "    'Observation': \"The 'test_in_positives' test function focuses on testing the behavior related to positive cases in the input data.\",\n",
       "    'Functions': ['test_in_positives'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_fast_functions.py'},\n",
       "   {'ID': '2.1',\n",
       "    'Title': 'Ensure Data File Loads as Expected',\n",
       "    'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "    'Observation': 'N/A',\n",
       "    'Functions': [],\n",
       "    'Evaluation': 'Not Satisfied',\n",
       "    'Score': 0,\n",
       "    'file': '../../../lightfm/tests/test_fast_functions.py'},\n",
       "   {'ID': '5.1',\n",
       "    'Title': 'Validate Model Input and Output Compatibility',\n",
       "    'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "    'Observation': 'The test functions validate the input data by checking different positions in the matrix and the expected outcomes.',\n",
       "    'Functions': ['test_in_positives'],\n",
       "    'Evaluation': 'Partially Satisfied',\n",
       "    'Score': 0.5,\n",
       "    'file': '../../../lightfm/tests/test_fast_functions.py'}],\n",
       "  'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='\\n            Your task is to answer each question in the checklist using only the provided test functions.\\n            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\\n            Output a JSON format:\\n                [{\\n                    \"ID\": \\n                    \"Title\":\\n                    \"Requirement\":\\n                    \"Observation\":\\n                    \"Functions\": [ ... ]\\n                    \"Evaluation\": Satisfied/Partially Satisfied/Not Satisfied\\n                    \"Score\": (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\\n                }]\\n        '), AIMessage(content='```json\\n[\\n    {\\n        \"ID\": \"1.1\",\\n        \"Title\": \"Write Descriptive Test Names\",\\n        \"Requirement\": \"Each test function should have a clear, descriptive name that accurately reflects the test\\'s purpose and the specific functionality or scenario it examines.\",\\n        \"Observation\": \"The test function \\'test_in_positives\\' has a descriptive name that indicates it tests for positive cases related to the input data.\",\\n        \"Functions\": [\"test_in_positives\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"1.2\",\\n        \"Title\": \"Keep Tests Focused\",\\n        \"Requirement\": \"Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\",\\n        \"Observation\": \"The \\'test_in_positives\\' test function focuses on testing the behavior related to positive cases in the input data.\",\\n        \"Functions\": [\"test_in_positives\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"2.1\",\\n        \"Title\": \"Ensure Data File Loads as Expected\",\\n        \"Requirement\": \"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\",\\n        \"Observation\": \"N/A\",\\n        \"Functions\": [],\\n        \"Evaluation\": \"Not Satisfied\",\\n        \"Score\": 0\\n    },\\n    {\\n        \"ID\": \"5.1\",\\n        \"Title\": \"Validate Model Input and Output Compatibility\",\\n        \"Requirement\": \"Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\",\\n        \"Observation\": \"The test functions validate the input data by checking different positions in the matrix and the expected outcomes.\",\\n        \"Functions\": [\"test_in_positives\"],\\n        \"Evaluation\": \"Partially Satisfied\",\\n        \"Score\": 0.5\\n    }\\n]\\n```')])},\n",
       " {'file': '../../../lightfm/tests/test_movielens.py',\n",
       "  'report': [{'ID': '1.1',\n",
       "    'Title': 'Write Descriptive Test Names',\n",
       "    'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "    'Observation': 'All test functions have descriptive names that reflect the purpose and scenario being tested.',\n",
       "    'Functions': ['test_movielens_accuracy',\n",
       "     'test_logistic_precision',\n",
       "     'test_bpr_precision',\n",
       "     'test_bpr_precision_multithreaded',\n",
       "     'test_warp_precision',\n",
       "     'test_warp_precision_high_interaction_values',\n",
       "     'test_bpr_precision_high_interaction_values',\n",
       "     'test_warp_precision_multithreaded',\n",
       "     'test_warp_precision_adadelta',\n",
       "     'test_warp_precision_adadelta_multithreaded',\n",
       "     'test_warp_precision_max_sampled',\n",
       "     'test_warp_kos_precision',\n",
       "     'test_warp_stability',\n",
       "     'test_movielens_genre_accuracy',\n",
       "     'test_get_representations',\n",
       "     'test_movielens_both_accuracy',\n",
       "     'test_movielens_accuracy_fit',\n",
       "     'test_movielens_accuracy_pickle',\n",
       "     'test_movielens_accuracy_resume',\n",
       "     'test_movielens_accuracy_sample_weights',\n",
       "     'test_movielens_accuracy_sample_weights_grad_accumulation',\n",
       "     'test_state_reset',\n",
       "     'test_user_supplied_features_accuracy',\n",
       "     'test_zeros_negative_accuracy',\n",
       "     'test_zero_weights_accuracy',\n",
       "     'test_hogwild_accuracy',\n",
       "     'test_movielens_excessive_regularization',\n",
       "     'test_overfitting',\n",
       "     'test_regularization',\n",
       "     'test_training_schedules',\n",
       "     'test_random_state_fixing',\n",
       "     'test_random_state_advanced',\n",
       "     'test_sklearn_cv'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_movielens.py'},\n",
       "   {'ID': '1.2',\n",
       "    'Title': 'Keep Tests Focused',\n",
       "    'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "    'Observation': 'The test functions focus on specific scenarios, using consistent mock data for each test to isolate and examine individual behaviors.',\n",
       "    'Functions': ['test_movielens_accuracy',\n",
       "     'test_logistic_precision',\n",
       "     'test_bpr_precision',\n",
       "     'test_bpr_precision_multithreaded',\n",
       "     'test_warp_precision',\n",
       "     'test_warp_precision_high_interaction_values',\n",
       "     'test_bpr_precision_high_interaction_values',\n",
       "     'test_warp_precision_multithreaded',\n",
       "     'test_warp_precision_adadelta',\n",
       "     'test_warp_precision_adadelta_multithreaded',\n",
       "     'test_warp_precision_max_sampled',\n",
       "     'test_warp_kos_precision',\n",
       "     'test_warp_stability',\n",
       "     'test_movielens_genre_accuracy',\n",
       "     'test_get_representations',\n",
       "     'test_movielens_both_accuracy',\n",
       "     'test_movielens_accuracy_fit',\n",
       "     'test_movielens_accuracy_pickle',\n",
       "     'test_movielens_accuracy_resume',\n",
       "     'test_movielens_accuracy_sample_weights',\n",
       "     'test_movielens_accuracy_sample_weights_grad_accumulation',\n",
       "     'test_state_reset',\n",
       "     'test_user_supplied_features_accuracy',\n",
       "     'test_zeros_negative_accuracy',\n",
       "     'test_zero_weights_accuracy',\n",
       "     'test_hogwild_accuracy',\n",
       "     'test_movielens_excessive_regularization',\n",
       "     'test_overfitting',\n",
       "     'test_regularization',\n",
       "     'test_training_schedules',\n",
       "     'test_random_state_fixing',\n",
       "     'test_random_state_advanced',\n",
       "     'test_sklearn_cv'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_movielens.py'},\n",
       "   {'ID': '2.1',\n",
       "    'Title': 'Ensure Data File Loads as Expected',\n",
       "    'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "    'Observation': None,\n",
       "    'Functions': [],\n",
       "    'Evaluation': 'Not Satisfied',\n",
       "    'Score': 0,\n",
       "    'file': '../../../lightfm/tests/test_movielens.py'},\n",
       "   {'ID': '5.1',\n",
       "    'Title': 'Validate Model Input and Output Compatibility',\n",
       "    'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "    'Observation': None,\n",
       "    'Functions': [],\n",
       "    'Evaluation': 'Not Satisfied',\n",
       "    'Score': 0,\n",
       "    'file': '../../../lightfm/tests/test_movielens.py'}],\n",
       "  'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='\\n            Your task is to answer each question in the checklist using only the provided test functions.\\n            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\\n            Output a JSON format:\\n                [{\\n                    \"ID\": \\n                    \"Title\":\\n                    \"Requirement\":\\n                    \"Observation\":\\n                    \"Functions\": [ ... ]\\n                    \"Evaluation\": Satisfied/Partially Satisfied/Not Satisfied\\n                    \"Score\": (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\\n                }]\\n        '), AIMessage(content='```json\\n[\\n    {\\n        \"ID\": \"1.1\",\\n        \"Title\": \"Write Descriptive Test Names\",\\n        \"Requirement\": \"Each test function should have a clear, descriptive name that accurately reflects the test\\'s purpose and the specific functionality or scenario it examines.\",\\n        \"Observation\": \"All test functions have descriptive names that reflect the purpose and scenario being tested.\",\\n        \"Functions\": [\\n            \"test_movielens_accuracy\",\\n            \"test_logistic_precision\",\\n            \"test_bpr_precision\",\\n            \"test_bpr_precision_multithreaded\",\\n            \"test_warp_precision\",\\n            \"test_warp_precision_high_interaction_values\",\\n            \"test_bpr_precision_high_interaction_values\",\\n            \"test_warp_precision_multithreaded\",\\n            \"test_warp_precision_adadelta\",\\n            \"test_warp_precision_adadelta_multithreaded\",\\n            \"test_warp_precision_max_sampled\",\\n            \"test_warp_kos_precision\",\\n            \"test_warp_stability\",\\n            \"test_movielens_genre_accuracy\",\\n            \"test_get_representations\",\\n            \"test_movielens_both_accuracy\",\\n            \"test_movielens_accuracy_fit\",\\n            \"test_movielens_accuracy_pickle\",\\n            \"test_movielens_accuracy_resume\",\\n            \"test_movielens_accuracy_sample_weights\",\\n            \"test_movielens_accuracy_sample_weights_grad_accumulation\",\\n            \"test_state_reset\",\\n            \"test_user_supplied_features_accuracy\",\\n            \"test_zeros_negative_accuracy\",\\n            \"test_zero_weights_accuracy\",\\n            \"test_hogwild_accuracy\",\\n            \"test_movielens_excessive_regularization\",\\n            \"test_overfitting\",\\n            \"test_regularization\",\\n            \"test_training_schedules\",\\n            \"test_random_state_fixing\",\\n            \"test_random_state_advanced\",\\n            \"test_sklearn_cv\"\\n        ],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"1.2\",\\n        \"Title\": \"Keep Tests Focused\",\\n        \"Requirement\": \"Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\",\\n        \"Observation\": \"The test functions focus on specific scenarios, using consistent mock data for each test to isolate and examine individual behaviors.\",\\n        \"Functions\": [\\n            \"test_movielens_accuracy\",\\n            \"test_logistic_precision\",\\n            \"test_bpr_precision\",\\n            \"test_bpr_precision_multithreaded\",\\n            \"test_warp_precision\",\\n            \"test_warp_precision_high_interaction_values\",\\n            \"test_bpr_precision_high_interaction_values\",\\n            \"test_warp_precision_multithreaded\",\\n            \"test_warp_precision_adadelta\",\\n            \"test_warp_precision_adadelta_multithreaded\",\\n            \"test_warp_precision_max_sampled\",\\n            \"test_warp_kos_precision\",\\n            \"test_warp_stability\",\\n            \"test_movielens_genre_accuracy\",\\n            \"test_get_representations\",\\n            \"test_movielens_both_accuracy\",\\n            \"test_movielens_accuracy_fit\",\\n            \"test_movielens_accuracy_pickle\",\\n            \"test_movielens_accuracy_resume\",\\n            \"test_movielens_accuracy_sample_weights\",\\n            \"test_movielens_accuracy_sample_weights_grad_accumulation\",\\n            \"test_state_reset\",\\n            \"test_user_supplied_features_accuracy\",\\n            \"test_zeros_negative_accuracy\",\\n            \"test_zero_weights_accuracy\",\\n            \"test_hogwild_accuracy\",\\n            \"test_movielens_excessive_regularization\",\\n            \"test_overfitting\",\\n            \"test_regularization\",\\n            \"test_training_schedules\",\\n            \"test_random_state_fixing\",\\n            \"test_random_state_advanced\",\\n            \"test_sklearn_cv\"\\n        ],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"2.1\",\\n        \"Title\": \"Ensure Data File Loads as Expected\",\\n        \"Requirement\": \"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\",\\n        \"Observation\": null,\\n        \"Functions\": [],\\n        \"Evaluation\": \"Not Satisfied\",\\n        \"Score\": 0\\n    },\\n    {\\n        \"ID\": \"5.1\",\\n        \"Title\": \"Validate Model Input and Output Compatibility\",\\n        \"Requirement\": \"Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\",\\n        \"Observation\": null,\\n        \"Functions\": [],\\n        \"Evaluation\": \"Not Satisfied\",\\n        \"Score\": 0\\n    }\\n]\\n```')])},\n",
       " {'file': '../../../lightfm/tests/test_datasets.py',\n",
       "  'report': [{'ID': '1.1',\n",
       "    'Title': 'Write Descriptive Test Names',\n",
       "    'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "    'Observation': \"The test functions have descriptive names that reflect the purpose and scenario being tested. For example, 'test_basic_fetching_movielens' and 'test_basic_fetching_stackexchange'.\",\n",
       "    'Functions': ['test_basic_fetching_movielens',\n",
       "     'test_basic_fetching_stackexchange'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_datasets.py'},\n",
       "   {'ID': '1.2',\n",
       "    'Title': 'Keep Tests Focused',\n",
       "    'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "    'Observation': \"The test functions focus on specific scenarios related to fetching data from 'movielens' and 'stackexchange'. Each test examines different aspects of the data fetching process.\",\n",
       "    'Functions': ['test_basic_fetching_movielens',\n",
       "     'test_basic_fetching_stackexchange'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_datasets.py'},\n",
       "   {'ID': '2.1',\n",
       "    'Title': 'Ensure Data File Loads as Expected',\n",
       "    'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "    'Observation': \"The test functions validate the loading of data files from 'movielens' and 'stackexchange' datasets. They check the types, shapes, and specific features of the loaded data.\",\n",
       "    'Functions': ['test_basic_fetching_movielens',\n",
       "     'test_basic_fetching_stackexchange'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_datasets.py'},\n",
       "   {'ID': '5.1',\n",
       "    'Title': 'Validate Model Input and Output Compatibility',\n",
       "    'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "    'Observation': \"The test functions validate the compatibility of model inputs and outputs by checking the shapes, types, and specific features of the fetched data from 'movielens' and 'stackexchange'.\",\n",
       "    'Functions': ['test_basic_fetching_movielens',\n",
       "     'test_basic_fetching_stackexchange'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_datasets.py'}],\n",
       "  'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='\\n            Your task is to answer each question in the checklist using only the provided test functions.\\n            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\\n            Output a JSON format:\\n                [{\\n                    \"ID\": \\n                    \"Title\":\\n                    \"Requirement\":\\n                    \"Observation\":\\n                    \"Functions\": [ ... ]\\n                    \"Evaluation\": Satisfied/Partially Satisfied/Not Satisfied\\n                    \"Score\": (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\\n                }]\\n        '), AIMessage(content='```json\\n[\\n    {\\n        \"ID\": \"1.1\",\\n        \"Title\": \"Write Descriptive Test Names\",\\n        \"Requirement\": \"Each test function should have a clear, descriptive name that accurately reflects the test\\'s purpose and the specific functionality or scenario it examines.\",\\n        \"Observation\": \"The test functions have descriptive names that reflect the purpose and scenario being tested. For example, \\'test_basic_fetching_movielens\\' and \\'test_basic_fetching_stackexchange\\'.\",\\n        \"Functions\": [\"test_basic_fetching_movielens\", \"test_basic_fetching_stackexchange\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"1.2\",\\n        \"Title\": \"Keep Tests Focused\",\\n        \"Requirement\": \"Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\",\\n        \"Observation\": \"The test functions focus on specific scenarios related to fetching data from \\'movielens\\' and \\'stackexchange\\'. Each test examines different aspects of the data fetching process.\",\\n        \"Functions\": [\"test_basic_fetching_movielens\", \"test_basic_fetching_stackexchange\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"2.1\",\\n        \"Title\": \"Ensure Data File Loads as Expected\",\\n        \"Requirement\": \"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\",\\n        \"Observation\": \"The test functions validate the loading of data files from \\'movielens\\' and \\'stackexchange\\' datasets. They check the types, shapes, and specific features of the loaded data.\",\\n        \"Functions\": [\"test_basic_fetching_movielens\", \"test_basic_fetching_stackexchange\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"5.1\",\\n        \"Title\": \"Validate Model Input and Output Compatibility\",\\n        \"Requirement\": \"Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\",\\n        \"Observation\": \"The test functions validate the compatibility of model inputs and outputs by checking the shapes, types, and specific features of the fetched data from \\'movielens\\' and \\'stackexchange\\'.\",\\n        \"Functions\": [\"test_basic_fetching_movielens\", \"test_basic_fetching_stackexchange\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    }\\n]\\n```')])},\n",
       " {'file': '../../../lightfm/tests/test_cross_validation.py',\n",
       "  'report': [{'ID': '1.1',\n",
       "    'Title': 'Write Descriptive Test Names',\n",
       "    'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "    'Observation': \"The test function 'test_random_train_test_split' has a clear and descriptive name that reflects its purpose of testing the random train-test split functionality.\",\n",
       "    'Functions': ['test_random_train_test_split'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_cross_validation.py'},\n",
       "   {'ID': '1.2',\n",
       "    'Title': 'Keep Tests Focused',\n",
       "    'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "    'Observation': \"The test function 'test_random_train_test_split' focuses on testing the random train-test split functionality with a specific test percentage.\",\n",
       "    'Functions': ['test_random_train_test_split'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_cross_validation.py'},\n",
       "   {'ID': '2.1',\n",
       "    'Title': 'Ensure Data File Loads as Expected',\n",
       "    'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "    'Observation': \"The test function 'test_random_train_test_split' fetches the MovieLens data and performs a random train-test split on it.\",\n",
       "    'Functions': ['test_random_train_test_split'],\n",
       "    'Evaluation': 'Partially Satisfied',\n",
       "    'Score': 0.5,\n",
       "    'file': '../../../lightfm/tests/test_cross_validation.py'},\n",
       "   {'ID': '5.1',\n",
       "    'Title': 'Validate Model Input and Output Compatibility',\n",
       "    'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "    'Observation': \"The test function 'test_random_train_test_split' validates the output of the random train-test split.\",\n",
       "    'Functions': ['test_random_train_test_split'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_cross_validation.py'}],\n",
       "  'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='\\n            Your task is to answer each question in the checklist using only the provided test functions.\\n            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\\n            Output a JSON format:\\n                [{\\n                    \"ID\": \\n                    \"Title\":\\n                    \"Requirement\":\\n                    \"Observation\":\\n                    \"Functions\": [ ... ]\\n                    \"Evaluation\": Satisfied/Partially Satisfied/Not Satisfied\\n                    \"Score\": (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\\n                }]\\n        '), AIMessage(content='```json\\n[\\n    {\\n        \"ID\": \"1.1\",\\n        \"Title\": \"Write Descriptive Test Names\",\\n        \"Requirement\": \"Each test function should have a clear, descriptive name that accurately reflects the test\\'s purpose and the specific functionality or scenario it examines.\",\\n        \"Observation\": \"The test function \\'test_random_train_test_split\\' has a clear and descriptive name that reflects its purpose of testing the random train-test split functionality.\",\\n        \"Functions\": [\"test_random_train_test_split\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"1.2\",\\n        \"Title\": \"Keep Tests Focused\",\\n        \"Requirement\": \"Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\",\\n        \"Observation\": \"The test function \\'test_random_train_test_split\\' focuses on testing the random train-test split functionality with a specific test percentage.\",\\n        \"Functions\": [\"test_random_train_test_split\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"2.1\",\\n        \"Title\": \"Ensure Data File Loads as Expected\",\\n        \"Requirement\": \"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\",\\n        \"Observation\": \"The test function \\'test_random_train_test_split\\' fetches the MovieLens data and performs a random train-test split on it.\",\\n        \"Functions\": [\"test_random_train_test_split\"],\\n        \"Evaluation\": \"Partially Satisfied\",\\n        \"Score\": 0.5\\n    },\\n    {\\n        \"ID\": \"5.1\",\\n        \"Title\": \"Validate Model Input and Output Compatibility\",\\n        \"Requirement\": \"Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\",\\n        \"Observation\": \"The test function \\'test_random_train_test_split\\' validates the output of the random train-test split.\",\\n        \"Functions\": [\"test_random_train_test_split\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    }\\n]\\n```  ')])},\n",
       " {'file': '../../../lightfm/tests/test_evaluation.py',\n",
       "  'report': [{'ID': '1.1',\n",
       "    'Title': 'Write Descriptive Test Names',\n",
       "    'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "    'Observation': 'The test functions have clear and descriptive names that reflect the purpose of the test, such as test_precision_at_k, test_recall_at_k, test_auc_score, etc.',\n",
       "    'Functions': ['test_precision_at_k',\n",
       "     'test_precision_at_k_with_ties',\n",
       "     'test_recall_at_k',\n",
       "     'test_auc_score',\n",
       "     'test_intersections_check'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_evaluation.py'},\n",
       "   {'ID': '1.2',\n",
       "    'Title': 'Keep Tests Focused',\n",
       "    'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "    'Observation': 'The test functions focus on specific scenarios like precision at k, recall at k, AUC score, and intersection checks, using relevant mock data for each scenario.',\n",
       "    'Functions': ['test_precision_at_k',\n",
       "     'test_precision_at_k_with_ties',\n",
       "     'test_recall_at_k',\n",
       "     'test_auc_score',\n",
       "     'test_intersections_check'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_evaluation.py'},\n",
       "   {'ID': '2.1',\n",
       "    'Title': 'Ensure Data File Loads as Expected',\n",
       "    'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "    'Observation': 'The provided test functions do not involve loading data files; instead, they generate synthetic data for testing the model evaluation functions.',\n",
       "    'Functions': [],\n",
       "    'Evaluation': 'Not Applicable',\n",
       "    'Score': 0,\n",
       "    'file': '../../../lightfm/tests/test_evaluation.py'},\n",
       "   {'ID': '5.1',\n",
       "    'Title': 'Validate Model Input and Output Compatibility',\n",
       "    'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "    'Observation': \"The test functions validate the model's input and output compatibility by testing precision, recall, AUC score, and intersection checks with the model's predictions and ground truth data.\",\n",
       "    'Functions': ['test_precision_at_k',\n",
       "     'test_precision_at_k_with_ties',\n",
       "     'test_recall_at_k',\n",
       "     'test_auc_score',\n",
       "     'test_intersections_check'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_evaluation.py'}],\n",
       "  'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='\\n            Your task is to answer each question in the checklist using only the provided test functions.\\n            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\\n            Output a JSON format:\\n                [{\\n                    \"ID\": \\n                    \"Title\":\\n                    \"Requirement\":\\n                    \"Observation\":\\n                    \"Functions\": [ ... ]\\n                    \"Evaluation\": Satisfied/Partially Satisfied/Not Satisfied\\n                    \"Score\": (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\\n                }]\\n        '), AIMessage(content='```json\\n[\\n    {\\n        \"ID\": \"1.1\",\\n        \"Title\": \"Write Descriptive Test Names\",\\n        \"Requirement\": \"Each test function should have a clear, descriptive name that accurately reflects the test\\'s purpose and the specific functionality or scenario it examines.\",\\n        \"Observation\": \"The test functions have clear and descriptive names that reflect the purpose of the test, such as test_precision_at_k, test_recall_at_k, test_auc_score, etc.\",\\n        \"Functions\": [\"test_precision_at_k\", \"test_precision_at_k_with_ties\", \"test_recall_at_k\", \"test_auc_score\", \"test_intersections_check\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"1.2\",\\n        \"Title\": \"Keep Tests Focused\",\\n        \"Requirement\": \"Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\",\\n        \"Observation\": \"The test functions focus on specific scenarios like precision at k, recall at k, AUC score, and intersection checks, using relevant mock data for each scenario.\",\\n        \"Functions\": [\"test_precision_at_k\", \"test_precision_at_k_with_ties\", \"test_recall_at_k\", \"test_auc_score\", \"test_intersections_check\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"2.1\",\\n        \"Title\": \"Ensure Data File Loads as Expected\",\\n        \"Requirement\": \"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\",\\n        \"Observation\": \"The provided test functions do not involve loading data files; instead, they generate synthetic data for testing the model evaluation functions.\",\\n        \"Functions\": [],\\n        \"Evaluation\": \"Not Applicable\",\\n        \"Score\": 0\\n    },\\n    {\\n        \"ID\": \"5.1\",\\n        \"Title\": \"Validate Model Input and Output Compatibility\",\\n        \"Requirement\": \"Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\",\\n        \"Observation\": \"The test functions validate the model\\'s input and output compatibility by testing precision, recall, AUC score, and intersection checks with the model\\'s predictions and ground truth data.\",\\n        \"Functions\": [\"test_precision_at_k\", \"test_precision_at_k_with_ties\", \"test_recall_at_k\", \"test_auc_score\", \"test_intersections_check\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    }\\n]\\n```')])},\n",
       " {'file': '../../../lightfm/tests/test_data.py',\n",
       "  'report': [{'ID': '1.1',\n",
       "    'Title': 'Write Descriptive Test Names',\n",
       "    'Requirement': \"Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\",\n",
       "    'Observation': 'The test functions have clear and descriptive names that reflect the specific functionality or scenario being tested.',\n",
       "    'Functions': ['test_fitting',\n",
       "     'test_fitting_no_identity',\n",
       "     'test_exceptions',\n",
       "     'test_build_features'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_data.py'},\n",
       "   {'ID': '1.2',\n",
       "    'Title': 'Keep Tests Focused',\n",
       "    'Requirement': 'Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.',\n",
       "    'Observation': 'Each test focuses on a single scenario, using specific mock data and testing one behavior or outcome.',\n",
       "    'Functions': ['test_fitting',\n",
       "     'test_fitting_no_identity',\n",
       "     'test_exceptions',\n",
       "     'test_build_features'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_data.py'},\n",
       "   {'ID': '2.1',\n",
       "    'Title': 'Ensure Data File Loads as Expected',\n",
       "    'Requirement': 'Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.',\n",
       "    'Observation': 'The provided test functions do not involve loading data files; hence, this requirement is not directly applicable.',\n",
       "    'Functions': [],\n",
       "    'Evaluation': 'Not Applicable',\n",
       "    'Score': 0,\n",
       "    'file': '../../../lightfm/tests/test_data.py'},\n",
       "   {'ID': '5.1',\n",
       "    'Title': 'Validate Model Input and Output Compatibility',\n",
       "    'Requirement': 'Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.',\n",
       "    'Observation': 'The test functions validate model input and output compatibility by checking shapes and types of user and item features.',\n",
       "    'Functions': ['test_fitting',\n",
       "     'test_fitting_no_identity',\n",
       "     'test_build_features'],\n",
       "    'Evaluation': 'Satisfied',\n",
       "    'Score': 1,\n",
       "    'file': '../../../lightfm/tests/test_data.py'}],\n",
       "  'history': InMemoryChatMessageHistory(messages=[HumanMessage(content='\\n            Your task is to answer each question in the checklist using only the provided test functions.\\n            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\\n            Output a JSON format:\\n                [{\\n                    \"ID\": \\n                    \"Title\":\\n                    \"Requirement\":\\n                    \"Observation\":\\n                    \"Functions\": [ ... ]\\n                    \"Evaluation\": Satisfied/Partially Satisfied/Not Satisfied\\n                    \"Score\": (1 for Satisfied / 0.5 for Partially Satisfied / 0 for Not Satisfied)\\n                }]\\n        '), AIMessage(content='```json\\n[\\n    {\\n        \"ID\": \"1.1\",\\n        \"Title\": \"Write Descriptive Test Names\",\\n        \"Requirement\": \"Each test function should have a clear, descriptive name that accurately reflects the test\\'s purpose and the specific functionality or scenario it examines.\",\\n        \"Observation\": \"The test functions have clear and descriptive names that reflect the specific functionality or scenario being tested.\",\\n        \"Functions\": [\"test_fitting\", \"test_fitting_no_identity\", \"test_exceptions\", \"test_build_features\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"1.2\",\\n        \"Title\": \"Keep Tests Focused\",\\n        \"Requirement\": \"Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\",\\n        \"Observation\": \"Each test focuses on a single scenario, using specific mock data and testing one behavior or outcome.\",\\n        \"Functions\": [\"test_fitting\", \"test_fitting_no_identity\", \"test_exceptions\", \"test_build_features\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    },\\n    {\\n        \"ID\": \"2.1\",\\n        \"Title\": \"Ensure Data File Loads as Expected\",\\n        \"Requirement\": \"Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\",\\n        \"Observation\": \"The provided test functions do not involve loading data files; hence, this requirement is not directly applicable.\",\\n        \"Functions\": [],\\n        \"Evaluation\": \"Not Applicable\",\\n        \"Score\": 0\\n    },\\n    {\\n        \"ID\": \"5.1\",\\n        \"Title\": \"Validate Model Input and Output Compatibility\",\\n        \"Requirement\": \"Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\",\\n        \"Observation\": \"The test functions validate model input and output compatibility by checking shapes and types of user and item features.\",\\n        \"Functions\": [\"test_fitting\", \"test_fitting_no_identity\", \"test_build_features\"],\\n        \"Evaluation\": \"Satisfied\",\\n        \"Score\": 1\\n    }\\n]\\n```')])}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff546a7f-7cf6-49ec-a512-df8d9d638167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completeness_score(score_format='fraction', verbose=False):\n",
    "    # report_df = pd.DataFrame(self.evaluation_result)['report'].explode('report').apply(pd.Series)\n",
    "    report_df = pd.DataFrame(result['result'])['report'].explode('report').apply(pd.Series)\n",
    "    report_df = report_df.rename(columns={\"file\": \"File Path\"})\n",
    "    report_df['Function References'] = report_df[['File Path', 'Functions']].to_dict(orient='records')\n",
    "    report_df['Observation'] = '(' + report_df['File Path'].apply(lambda x: os.path.split(x)[-1]) + ') ' + report_df['Observation']\n",
    "    report_df = report_df.groupby(['ID', 'Title']).agg({\n",
    "        'Requirement': ['max'],\n",
    "        'Score': ['max', 'count'],\n",
    "        'Observation': [list],\n",
    "        'Function References': [list],\n",
    "    })\n",
    "    report_df.columns = ['Requirement', 'is_Satisfied', 'n_files_tested', 'Observations', 'Function References']\n",
    "    # self.evaluation_report = report_df\n",
    "    result['report'] = report_df.reset_index()\n",
    "\n",
    "    if score_format == 'fraction':\n",
    "        score = f\"{report_df['is_Satisfied'].sum()}/{report_df['is_Satisfied'].count()}\"\n",
    "    elif score_format == 'number':\n",
    "        score = report_df['is_Satisfied'].sum()/report_df['is_Satisfied'].count()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Report:\")\n",
    "        print(report_df)\n",
    "        print()\n",
    "        print(f'Score: {score}')\n",
    "        print()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2804dc81-5496-413d-b3ab-63c96e2675af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                                          Title  \\\n",
      "0  1.1                   Write Descriptive Test Names   \n",
      "1  1.2                             Keep Tests Focused   \n",
      "2  2.1             Ensure Data File Loads as Expected   \n",
      "3  5.1  Validate Model Input and Output Compatibility   \n",
      "\n",
      "                                         Requirement  is_Satisfied  \\\n",
      "0  Each test function should have a clear, descri...           1.0   \n",
      "1  Each test should focus on a single scenario, u...           1.0   \n",
      "2  Ensure that data-loading functions correctly l...           1.0   \n",
      "3  Confirm that the model accepts inputs of the c...           1.0   \n",
      "\n",
      "   n_files_tested                                       Observations  \\\n",
      "0               6  [(test_fast_functions.py) The test function 't...   \n",
      "1               6  [(test_fast_functions.py) The test function 't...   \n",
      "2               6  [(test_fast_functions.py) Not Applicable, nan,...   \n",
      "3               6  [(test_fast_functions.py) The test functions v...   \n",
      "\n",
      "                                 Function References  \n",
      "0  [{'File Path': '../../../lightfm/tests/test_fa...  \n",
      "1  [{'File Path': '../../../lightfm/tests/test_fa...  \n",
      "2  [{'File Path': '../../../lightfm/tests/test_fa...  \n",
      "3  [{'File Path': '../../../lightfm/tests/test_fa...  \n"
     ]
    }
   ],
   "source": [
    "result['score'] = get_completeness_score()\n",
    "import json\n",
    "# result['report']['function_sources'] = result['report']['function_sources'].apply(lambda x: '\\n'.join([json.dumps(i) for i in x]))\n",
    "print(result['report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7053709f-f7bd-42a7-b02a-5b5946e169c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_evaluation_report_to_html(output_path):\n",
    "    if result['report'] is None:\n",
    "        raise NotImplementedError(\n",
    "            \"Evaluation report is not yet created. Please make sure the function `get_completeness_score` is run before calling this export function\"\n",
    "        )\n",
    "    else:\n",
    "        report_df = result['report']\n",
    "        # report_df['function_sources'] = report_df['function_sources'].apply(lambda x: '\\n'.join([json.dumps(i) for i in x]))\n",
    "        report_df['function_sources'] = pd.DataFrame(report_df['function_sources'])\n",
    "        return report_df\n",
    "        # with open(output_path, 'w') as f:\n",
    "        #     f.write(report_df.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa34e6d5-57d7-4f5c-aaac-7e23676d1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../report/evaluation_report.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26e87db9-219c-4087-acb6-bd0708d2df58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>is_Satisfied</th>\n",
       "      <th>n_files_tested</th>\n",
       "      <th>function_sources</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <th>Write Descriptive Test Names</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'File_Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <th>Keep Tests Focused</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'File_Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <th>Ensure Data File Loads as Expected</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'File_Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <th>Validate Model Input and Output Compatibility</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>[{'File_Path': '../../../lightfm/tests/test_fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   is_Satisfied  \\\n",
       "ID  Title                                                         \n",
       "1.1 Write Descriptive Test Names                            1.0   \n",
       "1.2 Keep Tests Focused                                      1.0   \n",
       "2.1 Ensure Data File Loads as Expected                      1.0   \n",
       "5.1 Validate Model Input and Output Compatibility           1.0   \n",
       "\n",
       "                                                   n_files_tested  \\\n",
       "ID  Title                                                           \n",
       "1.1 Write Descriptive Test Names                                6   \n",
       "1.2 Keep Tests Focused                                          6   \n",
       "2.1 Ensure Data File Loads as Expected                          6   \n",
       "5.1 Validate Model Input and Output Compatibility               6   \n",
       "\n",
       "                                                                                    function_sources  \n",
       "ID  Title                                                                                             \n",
       "1.1 Write Descriptive Test Names                   [{'File_Path': '../../../lightfm/tests/test_fa...  \n",
       "1.2 Keep Tests Focused                             [{'File_Path': '../../../lightfm/tests/test_fa...  \n",
       "2.1 Ensure Data File Loads as Expected             [{'File_Path': '../../../lightfm/tests/test_fa...  \n",
       "5.1 Validate Model Input and Output Compatibility  [{'File_Path': '../../../lightfm/tests/test_fa...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_evaluation_report_to_html(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7fff1e7-21e0-4f5d-98b8-703a849b6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, 'w') as f:\n",
    "    f.write(result['report'].to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ba74b-6ac9-4d6b-a11a-71f17a21614f",
   "metadata": {},
   "source": [
    "Incorporate the data, prompts and parameters, feed into OpenAI API for test runs and fetch responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc750f68-4fa7-4264-a270-2f3cc0ea667c",
   "metadata": {},
   "source": [
    "The evaluation will be based on 2 metrics calculated from the response:\n",
    "- Completeness Score distribution: The distribution of the `num_test_runs` completeness scores per each set of parameters\n",
    "- Consistency Score: Out of all `checklist` items, the proportion of results remain consistent among `num_test_runs` runs per each set of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "22ea8a4d-1f08-4f4d-9c80-4dc56ac16b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>test_no</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>checklist_demo_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checklist_demo_2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "test_no             1    2    3    4    5\n",
       "model_name                               \n",
       "checklist_demo_1  1.0  1.0  1.0  1.0  1.0\n",
       "checklist_demo_2  1.0  1.0  1.0  1.0  1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_evaluator.get_completeness_score_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd191f9e-6cb6-4964-b060-7976f1529edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# completeness_score_df.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "391b097d-df72-4490-a91d-7d0858852ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_no</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>consistency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">checklist_demo_1</th>\n",
       "      <th>1.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">checklist_demo_2</th>\n",
       "      <th>1.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "test_no                 1    2    3    4    5  consistency\n",
       "model_name       ID                                       \n",
       "checklist_demo_1 1.1  1.0  1.0  1.0  1.0  1.0         True\n",
       "                 1.2  1.0  1.0  1.0  1.0  1.0         True\n",
       "                 2.1  1.0  1.0  1.0  1.0  1.0         True\n",
       "                 5.1  1.0  1.0  1.0  1.0  1.0         True\n",
       "checklist_demo_2 1.1  1.0  1.0  1.0  1.0  1.0         True\n",
       "                 1.2  1.0  1.0  1.0  1.0  1.0         True\n",
       "                 2.1  1.0  1.0  1.0  1.0  1.0         True\n",
       "                 5.1  1.0  1.0  1.0  1.0  1.0         True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_evaluator.get_consistency_dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca45a592-7490-4f86-8357-0707ef81e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consistency_df.groupby(['model_name']).agg({'consistency': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c0b4b-ca68-4d44-9cb7-977e08abadb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
