{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ca06b23-a3fb-466f-baf4-dc34a2720e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PythonLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from modules.repo import Repository\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "589b42d3-4e71-4676-8892-4bf1346118ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEvaluator:\n",
    "    def __init__(self, repo_path=None):\n",
    "        self.repo = None\n",
    "        self.test_fps = [] # test file paths\n",
    "        self.py_splits = []\n",
    "\n",
    "        self.checklist = ''\n",
    "        self.system_message = []\n",
    "        self.model = 'gpt-4o'\n",
    "        self.temperature = 0.1\n",
    "        self.chain = None\n",
    "\n",
    "        # self.evaluation_message = \"\"\"\n",
    "        #     Evaluate whether the codes has fulfilled the requirements and deliver a completion score. Do not include a summary evaluation.\n",
    "        #     Desired JSON format:\n",
    "        #         {\n",
    "        #             \"Requirement Title\":\n",
    "        #             \"Requirement\":\n",
    "        #             \"Observation\":\n",
    "        #             \"Related Functions\": [ ... ]\n",
    "        #             \"Evaluation\": (1 for Fulfilled / 0.5 for Partially fulfilled / 0 for Not fulfilled)\n",
    "        #         }\n",
    "        # \"\"\"\n",
    "        self.evaluation_message = \"\"\"\n",
    "            Your task is to answer each question in the checklist using only the provided test functions.\n",
    "            If an answer to the question is provided, it must be annotated with a citation of the test function(s) in the Observation session.\n",
    "            Then, decide the completion score in a fraction format based on your answers. The denominator should be the number of checklist items.\n",
    "            Desired format:\n",
    "                Checklist Evaluation:\n",
    "                    Requirement Title:\n",
    "                    Requirement:\n",
    "                    Observation:\n",
    "                    Evaluation: Satisfied/Partially Satisfied/Not Satisfied\n",
    "                Completion Score: Number of satisfied requirements/Number of requirements\n",
    "                    Number of satisfied requirements:\n",
    "                    Number of partially satisfied requirements:\n",
    "                    Number of not satisfied requirements:\n",
    "        \"\"\"\n",
    "\n",
    "        if repo_path is not None:\n",
    "            self.load_repo(repo_path)\n",
    "        \n",
    "    def load_repo(self, repo_path):\n",
    "        self.repo = Repository(repo_path)\n",
    "        self.test_fps = self.repo.list_test_files()['Python']\n",
    "\n",
    "    def load_test_file(self, file_path, overwrite=True):\n",
    "        loader = PythonLoader(file_path)\n",
    "        py = loader.load()\n",
    "        py_splits = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_documents(py)\n",
    "        \n",
    "        if overwrite:\n",
    "            self.py_splits = py_splits\n",
    "        \n",
    "        return py_splits\n",
    "\n",
    "    # def load_all_test_files(self):\n",
    "    #     self.py_splits = []\n",
    "    #     for fp in self.test_fps:\n",
    "    #         self.py_splits += self.load_test_file(fp, overwrite=False)\n",
    "\n",
    "    def load_test_dir(self, dir_path):\n",
    "        loader = DirectoryLoader(\n",
    "            dir_path,\n",
    "            glob=\"**/*.py\", \n",
    "            show_progress=True, \n",
    "            loader_cls=PythonLoader\n",
    "        )\n",
    "        docs = loader.load()\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        self.py_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    def load_checklist(self):\n",
    "        self.checklist = \"\"\"\n",
    "            Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\n",
    "            Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\n",
    "            Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.\n",
    "            Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.\n",
    "            Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\n",
    "            Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.\n",
    "            Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.\n",
    "            Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.\n",
    "            Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.\n",
    "            Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.\n",
    "            Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\n",
    "            For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.\n",
    "            Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.\n",
    "            Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.\n",
    "            If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.\n",
    "            Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.\n",
    "        \"\"\"\n",
    "\n",
    "    def init_system_message(self):\n",
    "        if len(self.checklist) == 0:\n",
    "            self.load_checklist()\n",
    "            \n",
    "        self.system_message = [\n",
    "            (\"system\", \"You are a senior machine learning engineer who specializes in performing Machine Learning system testing. Extract and analyze the test functions from the codes:\\n\\n{context}\"),\n",
    "            (\"system\", f\"Here is the Machine Learning system testing checklist delimited by triple quotes '''{self.checklist}'''\")\n",
    "        ]\n",
    "\n",
    "    def init_chain(self, system_message=None, model=None):\n",
    "        if system_message is None:\n",
    "            if len(self.system_message) == 0:\n",
    "                self.init_system_message()\n",
    "            system_message = self.system_message\n",
    "        else:\n",
    "            self.system_message = system_message\n",
    "\n",
    "        if model is None:\n",
    "            model = self.model\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            system_message + [\n",
    "                MessagesPlaceholder(variable_name=\"messages\")\n",
    "            ]\n",
    "        )\n",
    "        chat = ChatOpenAI(model=model, temperature=self.temperature)\n",
    "\n",
    "        chain = create_stuff_documents_chain(chat, prompt)\n",
    "        self.chain = chain\n",
    "        return chain\n",
    "\n",
    "    def get_ai_response(self, message, context, history=None):\n",
    "        if self.chain is None:\n",
    "            self.init_chain()\n",
    "\n",
    "        if history is None:\n",
    "            history = ChatMessageHistory()\n",
    "\n",
    "        history.add_user_message(message)\n",
    "        \n",
    "        response = self.chain.invoke({\n",
    "            \"context\": context, \n",
    "            \"messages\": history.messages\n",
    "        })\n",
    "        history.add_ai_message(response)\n",
    "\n",
    "        return response, history\n",
    "\n",
    "    def evaluate(self):\n",
    "        return self.get_ai_response(\n",
    "            message=self.evaluation_message, \n",
    "            context=self.py_splits\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8c25bf0-91ac-4e08-853f-9cde9467e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestEvaluator(\"../../data/raw/openja/lightfm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29cd5180-aad2-4a3c-80f6-619e57772de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 2958.42it/s]\n"
     ]
    }
   ],
   "source": [
    "test.load_test_dir('../../data/raw/openja/lightfm/tests/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "507ce5ea-bf1c-4860-9ccb-32410e73ef10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test.load_test_file(test.test_fps[2])\n",
    "#test.load_all_test_files()\n",
    "len(test.py_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a22918ee-b441-4157-a545-f53f6e357b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "report, history = test.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a582e2aa-6dac-4d59-894e-2fa8f926b7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checklist Evaluation:\n",
      "\n",
      "Requirement Title: Clear and Descriptive Test Names\n",
      "Requirement: Each test function should have a clear, descriptive name that accurately reflects the test's purpose and the specific functionality or scenario it examines.\n",
      "Observation: The test functions have clear and descriptive names such as `test_in_positives`, `test_movielens_accuracy`, `test_logistic_precision`, `test_bpr_precision`, etc.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Single Scenario Focus\n",
      "Requirement: Each test should focus on a single scenario, using only one set of mock data and testing one specific behavior or outcome to ensure clarity and isolate issues.\n",
      "Observation: Each test function focuses on a single scenario, such as `test_movielens_accuracy` focusing on the accuracy of the model on the Movielens dataset, and `test_bpr_precision` focusing on the precision of the BPR loss function.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Focused and Narrow Assertions\n",
      "Requirement: Assertions within tests should be focused and narrow. Ensure you are only testing relevant behaviors of complex objects and not including unrelated assertions.\n",
      "Observation: Assertions are focused and narrow, such as in `test_movielens_accuracy` where the assertions check the ROC AUC score of the train and test predictions.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Close Modifications and Assertions\n",
      "Requirement: Keep any modifications to objects and the corresponding assertions close together in your tests to maintain readability and clearly show the cause-and-effect relationship.\n",
      "Observation: Modifications and assertions are kept close together, such as in `test_movielens_accuracy` where the model is trained and then immediately tested with assertions.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Data-Loading Functions\n",
      "Requirement: Ensure that data-loading functions correctly load files when they exist and match the expected format, handle non-existent files appropriately, and return the expected results.\n",
      "Observation: The `fetch_movielens` function is tested in `test_basic_fetching_movielens` to ensure it loads the data correctly.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Data and Figures Saving Functions\n",
      "Requirement: Verify that functions for saving data and figures perform write operations correctly, checking that the operation succeeds and the content matches the expected format.\n",
      "Observation: There are no test functions provided that verify saving data and figures.\n",
      "Evaluation: Not Satisfied\n",
      "\n",
      "Requirement Title: Non-Empty Data Files\n",
      "Requirement: Ensure all data files are non-empty and contain the necessary data required for further analysis or processing tasks.\n",
      "Observation: The `fetch_movielens` function is tested to ensure the data is non-empty in `test_basic_fetching_movielens`.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Data Format for Processing Algorithms\n",
      "Requirement: Verify that the data to be ingested matches the format expected by processing algorithms (like pd.DataFrame for CSVs or np.array for images) and adheres to the expected schema.\n",
      "Observation: The data format is verified in `test_basic_fetching_movielens` to ensure it matches the expected sparse matrix format.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Null Values and Outliers\n",
      "Requirement: Check that data files are free from unexpected null values and identify any outliers that could affect the analysis. Tests should explicitly state if null values are part of expected data.\n",
      "Observation: There are no test functions provided that explicitly check for null values and outliers.\n",
      "Evaluation: Not Satisfied\n",
      "\n",
      "Requirement Title: Fixed Input Produces Expected Output\n",
      "Requirement: Test that a fixed input to a function or model produces the expected output, focusing on one verification per test to ensure predictable behavior.\n",
      "Observation: The `test_movielens_accuracy` function tests that a fixed input produces the expected ROC AUC score.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Correct Input Shapes and Types\n",
      "Requirement: Confirm that the model accepts inputs of the correct shapes and types and produces outputs that meet the expected shapes and types without any errors.\n",
      "Observation: The `test_input_dtypes` function verifies that the model accepts various input shapes and types.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Model Weights Update Correctly\n",
      "Requirement: For parametric models, ensure that the model's weights update correctly per training iteration. For non-parametric models, verify that the data fits correctly into the model.\n",
      "Observation: The `test_training_schedules` function verifies that the model's weights update correctly.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Model Output Shape\n",
      "Requirement: Ensure the shape of the model's output aligns with the expected structure based on the task, such as matching the number of labels in a classification task.\n",
      "Observation: The `test_predict` function verifies that the model's output shape aligns with the expected structure.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Model Output Values\n",
      "Requirement: Verify that the model's output values are appropriate for its task, such as outputting probabilities that sum to 1 for classification tasks.\n",
      "Observation: The `test_predict` function verifies that the model's output values are appropriate.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Gradient Descent Step\n",
      "Requirement: If using gradient descent for training, verify that a single gradient step on a batch of data results in a decrease in the model's training loss.\n",
      "Observation: The `test_training_schedules` function indirectly verifies that gradient steps are taken correctly.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Requirement Title: Data Leakage\n",
      "Requirement: Confirm that there is no leakage of data between training, validation, and testing sets, or across cross-validation folds, to ensure the integrity of the splits.\n",
      "Observation: The `test_random_train_test_split` function verifies that there is no data leakage between training and testing sets.\n",
      "Evaluation: Satisfied\n",
      "\n",
      "Completion Score: 14/17\n",
      "Number of satisfied requirements: 14\n",
      "Number of partially satisfied requirements: 0\n",
      "Number of not satisfied requirements: 3\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce089084-6e0f-4cff-956c-3177f7fa80c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
