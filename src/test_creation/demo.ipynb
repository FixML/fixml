{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2c1ead7-9d5b-4414-80e2-07092ba180ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyze import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0a59a9-185c-4f17-a0dd-fa2534958ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:38<00:00, 12.94s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "checklist = Checklist('../../checklist/checklist_sys.csv', checklist_format=ChecklistFormat.CSV)\n",
    "extractor = PythonTestFileExtractor(Repository('../../data/raw/openja/lightfm_demo'))\n",
    "\n",
    "evaluator = TestEvaluator(llm, extractor, checklist)\n",
    "response = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5887705f-9308-4c7c-aef2-9571d3c8b6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "                                                     is_Satisfied  \\\n",
      "ID  Title                                                           \n",
      "2.1 Test Data Fetching and File Reading                       0.0   \n",
      "3.1 Validate Data Shape and Values                            0.0   \n",
      "3.2 Check for Duplicate Records in Data                       0.0   \n",
      "4.1 Verify Data Split Proportion                              0.5   \n",
      "5.1 Test Model Output Shape                                   0.5   \n",
      "6.1 Verify Evaluation Metrics Implementation                  0.5   \n",
      "6.2 Evaluate Model's Performance Against Thresholds           0.0   \n",
      "8.1 Validate Outliers Detection and Handling                  0.0   \n",
      "\n",
      "                                                     n_files_tested  \n",
      "ID  Title                                                            \n",
      "2.1 Test Data Fetching and File Reading                           3  \n",
      "3.1 Validate Data Shape and Values                                3  \n",
      "3.2 Check for Duplicate Records in Data                           3  \n",
      "4.1 Verify Data Split Proportion                                  3  \n",
      "5.1 Test Model Output Shape                                       3  \n",
      "6.1 Verify Evaluation Metrics Implementation                      3  \n",
      "6.2 Evaluate Model's Performance Against Thresholds               3  \n",
      "8.1 Validate Outliers Detection and Handling                      3  \n",
      "\n",
      "Score: 1.5/8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = ResponseParser(response)\n",
    "parser.get_completeness_score(verbose=True)\n",
    "parser.export_evaluation_report('report.html', 'html', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d717ba5d-dc9d-477d-a9db-ccb993f48f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "                                                   is_Satisfied  \\\n",
      "ID  Title                                                         \n",
      "1.1 Write Descriptive Test Names                            1.0   \n",
      "1.2 Keep Tests Focused                                      1.0   \n",
      "2.1 Ensure Data File Loads as Expected                      0.5   \n",
      "5.1 Validate Model Input and Output Compatibility           1.0   \n",
      "\n",
      "                                                   n_files_tested  \\\n",
      "ID  Title                                                           \n",
      "1.1 Write Descriptive Test Names                                3   \n",
      "1.2 Keep Tests Focused                                          3   \n",
      "2.1 Ensure Data File Loads as Expected                          3   \n",
      "5.1 Validate Model Input and Output Compatibility               3   \n",
      "\n",
      "                                                                                           functions  \n",
      "ID  Title                                                                                             \n",
      "1.1 Write Descriptive Test Names                   [test_random_train_test_split, test_precision_...  \n",
      "1.2 Keep Tests Focused                             [test_random_train_test_split, test_precision_...  \n",
      "2.1 Ensure Data File Loads as Expected                                [test_random_train_test_split]  \n",
      "5.1 Validate Model Input and Output Compatibility  [test_random_train_test_split, test_precision_...  \n",
      "\n",
      "Score: 3.5/4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = evaluator.get_completeness_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273db18c-13c4-4c86-a4c8-f42e0b0e37c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>file</th>\n",
       "      <th>Evaluation</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Validate Model Input and Output Compatibility</td>\n",
       "      <td>tests/test_cross_validation.py</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Validate Model Input and Output Compatibility</td>\n",
       "      <td>tests/test_evaluation.py</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Validate Model Input and Output Compatibility</td>\n",
       "      <td>tests/test_data.py</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                          Title  \\\n",
       "3   5.1  Validate Model Input and Output Compatibility   \n",
       "7   5.1  Validate Model Input and Output Compatibility   \n",
       "11  5.1  Validate Model Input and Output Compatibility   \n",
       "\n",
       "                              file Evaluation  Score  \n",
       "3   tests/test_cross_validation.py  Satisfied    1.0  \n",
       "7         tests/test_evaluation.py  Satisfied    1.0  \n",
       "11              tests/test_data.py  Satisfied    1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports = pd.DataFrame(evaluator.evaluation_result)['report'].explode('report').apply(pd.Series)\n",
    "reports['file'] = reports['file'].str[35:]\n",
    "reports.query('ID == \"5.1\"')[['ID', 'Title', 'file', 'Evaluation', 'Score']]#.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a682a42-8807-48c6-9de4-0558838e3ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>file</th>\n",
       "      <th>Evaluation</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>tests/test_cross_validation.py</td>\n",
       "      <td>Partially Satisfied</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>tests/test_evaluation.py</td>\n",
       "      <td>Not Satisfied</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>tests/test_data.py</td>\n",
       "      <td>Not Satisfied</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                               Title                            file  \\\n",
       "2   2.1  Ensure Data File Loads as Expected  tests/test_cross_validation.py   \n",
       "6   2.1  Ensure Data File Loads as Expected        tests/test_evaluation.py   \n",
       "10  2.1  Ensure Data File Loads as Expected              tests/test_data.py   \n",
       "\n",
       "             Evaluation  Score  \n",
       "2   Partially Satisfied    0.5  \n",
       "6         Not Satisfied    0.0  \n",
       "10        Not Satisfied    0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports.query('ID == \"2.1\"')[['ID', 'Title', 'file', 'Evaluation', 'Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889fd144-c4c1-4365-81f5-317f3cf6c4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Requirement</th>\n",
       "      <th>Observation</th>\n",
       "      <th>Functions</th>\n",
       "      <th>Evaluation</th>\n",
       "      <th>Score</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>Write Descriptive Test Names</td>\n",
       "      <td>Each test function should have a clear, descri...</td>\n",
       "      <td>The test function 'test_random_train_test_spli...</td>\n",
       "      <td>[test_random_train_test_split]</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_cross_validation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.2</td>\n",
       "      <td>Keep Tests Focused</td>\n",
       "      <td>Each test should focus on a single scenario, u...</td>\n",
       "      <td>The test function 'test_random_train_test_spli...</td>\n",
       "      <td>[test_random_train_test_split]</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_cross_validation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>Ensure that data-loading functions correctly l...</td>\n",
       "      <td>The test function 'test_random_train_test_spli...</td>\n",
       "      <td>[test_random_train_test_split]</td>\n",
       "      <td>Partially Satisfied</td>\n",
       "      <td>0.5</td>\n",
       "      <td>tests/test_cross_validation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Validate Model Input and Output Compatibility</td>\n",
       "      <td>Confirm that the model accepts inputs of the c...</td>\n",
       "      <td>The test function 'test_random_train_test_spli...</td>\n",
       "      <td>[test_random_train_test_split]</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_cross_validation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.1</td>\n",
       "      <td>Write Descriptive Test Names</td>\n",
       "      <td>Each test function should have a clear, descri...</td>\n",
       "      <td>The test functions have clear and descriptive ...</td>\n",
       "      <td>[test_precision_at_k, test_precision_at_k_with...</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_evaluation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.2</td>\n",
       "      <td>Keep Tests Focused</td>\n",
       "      <td>Each test should focus on a single scenario, u...</td>\n",
       "      <td>Each test focuses on a single scenario, using ...</td>\n",
       "      <td>[test_precision_at_k, test_precision_at_k_with...</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_evaluation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>Ensure that data-loading functions correctly l...</td>\n",
       "      <td>The provided test functions do not involve loa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Not Satisfied</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tests/test_evaluation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Validate Model Input and Output Compatibility</td>\n",
       "      <td>Confirm that the model accepts inputs of the c...</td>\n",
       "      <td>The test functions validate the model's input ...</td>\n",
       "      <td>[test_precision_at_k, test_precision_at_k_with...</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_evaluation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.1</td>\n",
       "      <td>Write Descriptive Test Names</td>\n",
       "      <td>Each test function should have a clear, descri...</td>\n",
       "      <td>The test functions have clear and descriptive ...</td>\n",
       "      <td>[test_fitting, test_fitting_no_identity, test_...</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_data.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.2</td>\n",
       "      <td>Keep Tests Focused</td>\n",
       "      <td>Each test should focus on a single scenario, u...</td>\n",
       "      <td>The test functions focus on specific scenarios...</td>\n",
       "      <td>[test_fitting, test_fitting_no_identity, test_...</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_data.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.1</td>\n",
       "      <td>Ensure Data File Loads as Expected</td>\n",
       "      <td>Ensure that data-loading functions correctly l...</td>\n",
       "      <td>The test functions do not directly involve dat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Not Satisfied</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tests/test_data.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.1</td>\n",
       "      <td>Validate Model Input and Output Compatibility</td>\n",
       "      <td>Confirm that the model accepts inputs of the c...</td>\n",
       "      <td>The test functions validate the shapes and typ...</td>\n",
       "      <td>[test_fitting, test_fitting_no_identity, test_...</td>\n",
       "      <td>Satisfied</td>\n",
       "      <td>1.0</td>\n",
       "      <td>tests/test_data.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                          Title  \\\n",
       "0   1.1                   Write Descriptive Test Names   \n",
       "1   1.2                             Keep Tests Focused   \n",
       "2   2.1             Ensure Data File Loads as Expected   \n",
       "3   5.1  Validate Model Input and Output Compatibility   \n",
       "4   1.1                   Write Descriptive Test Names   \n",
       "5   1.2                             Keep Tests Focused   \n",
       "6   2.1             Ensure Data File Loads as Expected   \n",
       "7   5.1  Validate Model Input and Output Compatibility   \n",
       "8   1.1                   Write Descriptive Test Names   \n",
       "9   1.2                             Keep Tests Focused   \n",
       "10  2.1             Ensure Data File Loads as Expected   \n",
       "11  5.1  Validate Model Input and Output Compatibility   \n",
       "\n",
       "                                          Requirement  \\\n",
       "0   Each test function should have a clear, descri...   \n",
       "1   Each test should focus on a single scenario, u...   \n",
       "2   Ensure that data-loading functions correctly l...   \n",
       "3   Confirm that the model accepts inputs of the c...   \n",
       "4   Each test function should have a clear, descri...   \n",
       "5   Each test should focus on a single scenario, u...   \n",
       "6   Ensure that data-loading functions correctly l...   \n",
       "7   Confirm that the model accepts inputs of the c...   \n",
       "8   Each test function should have a clear, descri...   \n",
       "9   Each test should focus on a single scenario, u...   \n",
       "10  Ensure that data-loading functions correctly l...   \n",
       "11  Confirm that the model accepts inputs of the c...   \n",
       "\n",
       "                                          Observation  \\\n",
       "0   The test function 'test_random_train_test_spli...   \n",
       "1   The test function 'test_random_train_test_spli...   \n",
       "2   The test function 'test_random_train_test_spli...   \n",
       "3   The test function 'test_random_train_test_spli...   \n",
       "4   The test functions have clear and descriptive ...   \n",
       "5   Each test focuses on a single scenario, using ...   \n",
       "6   The provided test functions do not involve loa...   \n",
       "7   The test functions validate the model's input ...   \n",
       "8   The test functions have clear and descriptive ...   \n",
       "9   The test functions focus on specific scenarios...   \n",
       "10  The test functions do not directly involve dat...   \n",
       "11  The test functions validate the shapes and typ...   \n",
       "\n",
       "                                            Functions           Evaluation  \\\n",
       "0                      [test_random_train_test_split]            Satisfied   \n",
       "1                      [test_random_train_test_split]            Satisfied   \n",
       "2                      [test_random_train_test_split]  Partially Satisfied   \n",
       "3                      [test_random_train_test_split]            Satisfied   \n",
       "4   [test_precision_at_k, test_precision_at_k_with...            Satisfied   \n",
       "5   [test_precision_at_k, test_precision_at_k_with...            Satisfied   \n",
       "6                                                  []        Not Satisfied   \n",
       "7   [test_precision_at_k, test_precision_at_k_with...            Satisfied   \n",
       "8   [test_fitting, test_fitting_no_identity, test_...            Satisfied   \n",
       "9   [test_fitting, test_fitting_no_identity, test_...            Satisfied   \n",
       "10                                                 []        Not Satisfied   \n",
       "11  [test_fitting, test_fitting_no_identity, test_...            Satisfied   \n",
       "\n",
       "    Score                            file  \n",
       "0     1.0  tests/test_cross_validation.py  \n",
       "1     1.0  tests/test_cross_validation.py  \n",
       "2     0.5  tests/test_cross_validation.py  \n",
       "3     1.0  tests/test_cross_validation.py  \n",
       "4     1.0        tests/test_evaluation.py  \n",
       "5     1.0        tests/test_evaluation.py  \n",
       "6     0.0        tests/test_evaluation.py  \n",
       "7     1.0        tests/test_evaluation.py  \n",
       "8     1.0              tests/test_data.py  \n",
       "9     1.0              tests/test_data.py  \n",
       "10    0.0              tests/test_data.py  \n",
       "11    1.0              tests/test_data.py  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07875448-9c58-4ec0-94b8-de9be8870011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report:\n",
      "                                                     is_Satisfied  \\\n",
      "ID  Title                                                           \n",
      "2.1 Test Data Fetching and File Reading                       0.0   \n",
      "3.1 Validate Data Shape and Values                            0.0   \n",
      "3.2 Check for Duplicate Records in Data                       0.0   \n",
      "4.1 Verify Data Split Proportion                              1.0   \n",
      "5.1 Test Model Output Shape                                   0.5   \n",
      "6.1 Verify Evaluation Metrics Implementation                  0.5   \n",
      "6.2 Evaluate Model's Performance Against Thresholds           0.5   \n",
      "8.1 Validate Outliers Detection and Handling                  0.0   \n",
      "\n",
      "                                                     n_files_tested  \n",
      "ID  Title                                                            \n",
      "2.1 Test Data Fetching and File Reading                           2  \n",
      "3.1 Validate Data Shape and Values                                2  \n",
      "3.2 Check for Duplicate Records in Data                           2  \n",
      "4.1 Verify Data Split Proportion                                  3  \n",
      "5.1 Test Model Output Shape                                       2  \n",
      "6.1 Verify Evaluation Metrics Implementation                      2  \n",
      "6.2 Evaluate Model's Performance Against Thresholds               2  \n",
      "8.1 Validate Outliers Detection and Handling                      2  \n",
      "\n",
      "Score: 2.5/8\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.5/8'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ResponseParser(response)\n",
    "parser.get_completeness_score(verbose=True)\n",
    "#parser.export_evaluation_report(report_output_path, report_output_format, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b2e5a4-76fb-449c-8745-d5538302d20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Test Evaluation Report\\n\\n## Summary\\n\\n**Completeness Score**: 2.5/8\\n\\n**Completeness Score per Checklist Item**: \\n\\n|   ID | Title                                           |   is_Satisfied |   n_files_tested |\\n|-----:|:------------------------------------------------|---------------:|-----------------:|\\n|  2.1 | Test Data Fetching and File Reading             |            0   |                2 |\\n|  3.1 | Validate Data Shape and Values                  |            0   |                2 |\\n|  3.2 | Check for Duplicate Records in Data             |            0   |                2 |\\n|  4.1 | Verify Data Split Proportion                    |            1   |                3 |\\n|  5.1 | Test Model Output Shape                         |            0.5 |                2 |\\n|  6.1 | Verify Evaluation Metrics Implementation        |            0.5 |                2 |\\n|  6.2 | Evaluate Model's Performance Against Thresholds |            0.5 |                2 |\\n|  8.1 | Validate Outliers Detection and Handling        |            0   |                2 |\\n\\n## Details\\n\\n### 2.1 Test Data Fetching and File Reading\\n\\n**Requirement**: Verify that the data fetching API or data file reading functionality works correctly. Ensure that proper error handling is in place for scenarios such as missing files, incorrect file formats, and network errors.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not directly address data fetching or file reading. It focuses on generating synthetic data for testing.\\n  - (test_data.py) The code does not involve data fetching or file reading operations.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': [], 'Line Numbers': []}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': [], 'Line Numbers': []}\\n\\n### 3.1 Validate Data Shape and Values\\n\\n**Requirement**: Check that the data has the expected shape and that all values meet domain-specific constraints, such as non-negative distances.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not validate data shape or values. It focuses on generating synthetic data for testing.\\n  - (test_data.py) The code does not validate data shape or values.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': [], 'Line Numbers': []}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': [], 'Line Numbers': []}\\n\\n### 3.2 Check for Duplicate Records in Data\\n\\n**Requirement**: Check for duplicate records in the dataset and ensure that there are none.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not check for duplicate records in the dataset.\\n  - (test_data.py) The code does not check for duplicate records in the data.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': [], 'Line Numbers': []}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': [], 'Line Numbers': []}\\n\\n### 4.1 Verify Data Split Proportion\\n\\n**Requirement**: Check that the data is split into training and testing sets in the expected proportion.\\n\\n**Observations:**\\n\\n  - (test_cross_validation.py) The code includes a test function test_random_train_test_split that verifies the data split proportion.\\n  - (test_evaluation.py) The code generates synthetic data for testing but does not explicitly verify the data split proportion.\\n  - (test_data.py) The code does not verify data split proportion.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_cross_validation.py', 'Functions': ['test_random_train_test_split'], 'Line Numbers': [17]}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': [], 'Line Numbers': []}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': [], 'Line Numbers': []}\\n\\n### 5.1 Test Model Output Shape\\n\\n**Requirement**: Validate that the model's output has the expected shape.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not directly test the model's output shape.\\n  - (test_data.py) The code validates the model's output shape.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': [], 'Line Numbers': []}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': ['test_fitting'], 'Line Numbers': [7]}\\n\\n### 6.1 Verify Evaluation Metrics Implementation\\n\\n**Requirement**: Verify that the evaluation metrics are correctly implemented and appropriate for the model's task.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code includes alternative test implementations for precision, recall, and AUC metrics.\\n  - (test_data.py) The code does not verify evaluation metrics implementation.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': ['_precision_at_k', '_recall_at_k', '_auc'], 'Line Numbers': [34, 78, 122]}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': [], 'Line Numbers': []}\\n\\n### 6.2 Evaluate Model's Performance Against Thresholds\\n\\n**Requirement**: Compute evaluation metrics for both the training and testing datasets and ensure that these metrics exceed predefined threshold values, indicating acceptable model performance.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code includes tests for precision, recall, and AUC metrics but does not explicitly compare them against predefined threshold values.\\n  - (test_data.py) The code does not evaluate model performance against thresholds.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': [], 'Line Numbers': []}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': [], 'Line Numbers': []}\\n\\n### 8.1 Validate Outliers Detection and Handling\\n\\n**Requirement**: Detect outliers in the dataset. Ensure that the outlier detection mechanism is sensitive enough to flag true outliers while ignoring minor anomalies.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not address outliers detection and handling.\\n  - (test_data.py) The code does not validate outliers detection and handling.\\n\\n**Function References:**\\n\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_evaluation.py', 'Functions': [], 'Line Numbers': []}\\n  - {'File Path': '../../data/raw/openja/lightfm_demo/tests/test_data.py', 'Functions': [], 'Line Numbers': []}\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = parser.as_markdown()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061d9d78-843e-4b70-9a45-12912792bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Test Evaluation Report\\n\\n## Summary\\n\\n**Completeness Score**: 2.5/8\\n\\n**Completeness Score per Checklist Item**: \\n\\n|   ID | Title                                           |   is_Satisfied |   n_files_tested |\\n|-----:|:------------------------------------------------|---------------:|-----------------:|\\n|  2.1 | Test Data Fetching and File Reading             |            0   |                2 |\\n|  3.1 | Validate Data Shape and Values                  |            0   |                2 |\\n|  3.2 | Check for Duplicate Records in Data             |            0   |                2 |\\n|  4.1 | Verify Data Split Proportion                    |            1   |                3 |\\n|  5.1 | Test Model Output Shape                         |            0.5 |                2 |\\n|  6.1 | Verify Evaluation Metrics Implementation        |            0.5 |                2 |\\n|  6.2 | Evaluate Model\\'s Performance Against Thresholds |            0.5 |                2 |\\n|  8.1 | Validate Outliers Detection and Handling        |            0   |                2 |\\n\\n## Details\\n\\n### 2.1 Test Data Fetching and File Reading\\n\\n**Requirement**: Verify that the data fetching API or data file reading functionality works correctly. Ensure that proper error handling is in place for scenarios such as missing files, incorrect file formats, and network errors.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not directly address data fetching or file reading. It focuses on generating synthetic data for testing.\\n  - (test_data.py) The code does not involve data fetching or file reading operations.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n\\n### 3.1 Validate Data Shape and Values\\n\\n**Requirement**: Check that the data has the expected shape and that all values meet domain-specific constraints, such as non-negative distances.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not validate data shape or values. It focuses on generating synthetic data for testing.\\n  - (test_data.py) The code does not validate data shape or values.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n\\n### 3.2 Check for Duplicate Records in Data\\n\\n**Requirement**: Check for duplicate records in the dataset and ensure that there are none.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not check for duplicate records in the dataset.\\n  - (test_data.py) The code does not check for duplicate records in the data.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n\\n### 4.1 Verify Data Split Proportion\\n\\n**Requirement**: Check that the data is split into training and testing sets in the expected proportion.\\n\\n**Observations:**\\n\\n  - (test_cross_validation.py) The code includes a test function test_random_train_test_split that verifies the data split proportion.\\n  - (test_evaluation.py) The code generates synthetic data for testing but does not explicitly verify the data split proportion.\\n  - (test_data.py) The code does not verify data split proportion.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_cross_validation.py\\', \\'Functions\\': [\\'test_random_train_test_split\\'], \\'Line Numbers\\': [\"[17](https://github.com/lyst/lightfm/blob/master/tests/test_cross_validation.py#L17)\"]}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n\\n### 5.1 Test Model Output Shape\\n\\n**Requirement**: Validate that the model\\'s output has the expected shape.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not directly test the model\\'s output shape.\\n  - (test_data.py) The code validates the model\\'s output shape.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [\\'test_fitting\\'], \\'Line Numbers\\': [7]}\\n\\n### 6.1 Verify Evaluation Metrics Implementation\\n\\n**Requirement**: Verify that the evaluation metrics are correctly implemented and appropriate for the model\\'s task.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code includes alternative test implementations for precision, recall, and AUC metrics.\\n  - (test_data.py) The code does not verify evaluation metrics implementation.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [\\'_precision_at_k\\', \\'_recall_at_k\\', \\'_auc\\'], \\'Line Numbers\\': [34, 78, 122]}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n\\n### 6.2 Evaluate Model\\'s Performance Against Thresholds\\n\\n**Requirement**: Compute evaluation metrics for both the training and testing datasets and ensure that these metrics exceed predefined threshold values, indicating acceptable model performance.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code includes tests for precision, recall, and AUC metrics but does not explicitly compare them against predefined threshold values.\\n  - (test_data.py) The code does not evaluate model performance against thresholds.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n\\n### 8.1 Validate Outliers Detection and Handling\\n\\n**Requirement**: Detect outliers in the dataset. Ensure that the outlier detection mechanism is sensitive enough to flag true outliers while ignoring minor anomalies.\\n\\n**Observations:**\\n\\n  - (test_evaluation.py) The code does not address outliers detection and handling.\\n  - (test_data.py) The code does not validate outliers detection and handling.\\n\\n**Function References:**\\n\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_evaluation.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n  - {\\'File Path\\': \\'../../data/raw/openja/lightfm_demo/tests/test_data.py\\', \\'Functions\\': [], \\'Line Numbers\\': []}\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = tmp.replace('17', '\"[17](https://github.com/lyst/lightfm/blob/master/tests/test_cross_validation.py#L17)\"')\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a980ec3-b4bf-41ce-ad27-acf04eb076c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pypandoc\n",
    "pypandoc.convert_text(tmp.replace(\"'\", \"\\\\'\"), 'html', format='md', outputfile='test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457726d8-b0e7-4017-9676-6dba16b18eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-creation]",
   "language": "python",
   "name": "conda-env-test-creation-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
